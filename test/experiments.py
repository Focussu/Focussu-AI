import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from collections import defaultdict
import pandas as pd
from torch.utils.data import DataLoader
from model.PointNet import PointNetClassifier
from train.data import FocusDataset_multi


import matplotlib.font_manager
font_list = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
[matplotlib.font_manager.FontProperties(fname=font).get_name() for font in font_list if 'Nanum' in font]
\
import matplotlib.pyplot as plt
plt.rc('font', family='NanumGothicCoding')
\
import matplotlib as mpl
mpl.rcParams['axes.unicode_minus'] = False


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
base_path = '/shared_data/focussu/109.í•™ìŠµíƒœë„_ë°_ì„±í–¥_ê´€ì°°_ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°/Validation'
MODEL_PATH = 'model/best_multi_model.pth'

# ë¼ë²¨ ì •ì˜
LABEL_NAMES = {
    0: "ì§‘ì¤‘(í¥ë¯¸ë¡œì›€)",
    1: "ì§‘ì¤‘(ì°¨ë¶„í•¨)", 
    2: "ë¹„ì§‘ì¤‘(ì°¨ë¶„í•¨)",
    3: "ë¹„ì§‘ì¤‘(ì§€ë£¨í•¨)",
    4: "ì¡¸ìŒ"
}

def evaluate_model_performance(model, dataloader, device):
    """ëª¨ë¸ì˜ ì¢…í•©ì ì¸ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ë³´ê³ ì„œ ìƒì„±"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_logits = []
    all_formats = []  # format ì •ë³´ ì €ì¥
    class_correct = defaultdict(int)
    class_total = defaultdict(int)
    
    print("ëª¨ë¸ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            landmarks = batch['landmarks'].to(device)
            labels = batch['label'].long().to(device)
            formats = batch['format']  # format ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            logits = model(landmarks)
            _, predicted = torch.max(logits.data, 1)
            
            # ê²°ê³¼ ì €ì¥
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_logits.extend(torch.softmax(logits, dim=1).cpu().numpy())
            all_formats.extend(formats)  # format ì •ë³´ ì €ì¥
            
            # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            for i in range(labels.size(0)):
                label = labels[i].item()
                class_total[label] += 1
                if predicted[i] == labels[i]:
                    class_correct[label] += 1
            
            if batch_idx % 10 == 0:
                print(f"ë°°ì¹˜ {batch_idx}/{len(dataloader)} ì²˜ë¦¬ ì™„ë£Œ")
    
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_logits = np.array(all_logits)
    
    print(f"\nì´ {len(all_predictions)}ê°œ ìƒ˜í”Œ í‰ê°€ ì™„ë£Œ")
    
    # 1. ì „ì²´ ì •í™•ë„
    overall_accuracy = accuracy_score(all_labels, all_predictions)
    
    # 2. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_accuracies = {}
    for class_id in range(5):
        if class_total[class_id] > 0:
            class_accuracies[class_id] = class_correct[class_id] / class_total[class_id]
        else:
            class_accuracies[class_id] = 0.0
    
    # 3. Precision, Recall, F1-score
    precision, recall, f1, support = precision_recall_fscore_support(
        all_labels, all_predictions, average=None, zero_division=0
    )
    
    # 4. Confusion Matrix
    cm = confusion_matrix(all_labels, all_predictions)
    
    # 5. Classification Report
    class_report = classification_report(
        all_labels, all_predictions, 
        target_names=[LABEL_NAMES[i] for i in range(5)],
        zero_division=0
    )
    
    return {
        'overall_accuracy': overall_accuracy,
        'class_accuracies': class_accuracies,
        'class_total': dict(class_total),
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'support': support,
        'confusion_matrix': cm,
        'classification_report': class_report,
        'predictions': all_predictions,
        'labels': all_labels,
        'logits': all_logits,
        'formats': all_formats  # format ì •ë³´ ì¶”ê°€
    }

def print_performance_report(results):
    """ì„±ëŠ¥ ì§€í‘œ ë³´ê³ ì„œë¥¼ ì¶œë ¥"""
    print("=" * 80)
    print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë³´ê³ ì„œ")
    print("=" * 80)
    
    # ì „ì²´ ì •í™•ë„
    print(f"\nğŸ“Š ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f} ({results['overall_accuracy']*100:.2f}%)")
    
    
    # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì •ë³´
    print(f"\nğŸ“‹ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ:")
    print("-" * 80)
    print(f"{'í´ë˜ìŠ¤':<15} {'ìƒ˜í”Œìˆ˜':<8} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1ì ìˆ˜':<8}")
    print("-" * 80)
    
    for i in range(5):
        class_name = LABEL_NAMES[i]
        sample_count = results['class_total'].get(i, 0)
        accuracy = results['class_accuracies'][i]
        precision = results['precision'][i]
        recall = results['recall'][i]
        f1 = results['f1_score'][i]
        
        print(f"{class_name:<15} {sample_count:<8} {accuracy:<8.4f} {precision:<8.4f} {recall:<8.4f} {f1:<8.4f}")
    
    # í‰ê·  ì§€í‘œ
    print("-" * 80)
    avg_precision = np.mean(results['precision'])
    avg_recall = np.mean(results['recall'])
    avg_f1 = np.mean(results['f1_score'])
    
    print(f"{'í‰ê· ':<15} {'':<8} {'':<8} {avg_precision:<8.4f} {avg_recall:<8.4f} {avg_f1:<8.4f}")
    
    # ê°€ì¤‘ í‰ê·  ì§€í‘œ
    weighted_precision = np.average(results['precision'], weights=results['support'])
    weighted_recall = np.average(results['recall'], weights=results['support'])
    weighted_f1 = np.average(results['f1_score'], weights=results['support'])
    
    print(f"{'ê°€ì¤‘í‰ê· ':<15} {'':<8} {'':<8} {weighted_precision:<8.4f} {weighted_recall:<8.4f} {weighted_f1:<8.4f}")
    
    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
    print(f"\nğŸ“ˆ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
    print(results['classification_report'])

def plot_confusion_matrix(cm, save_path='confusion_matrix.png'):
    """Confusion Matrix ì‹œê°í™”"""
    # ë™ì  ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í°íŠ¸ ì„¤ì •ë„ í¬í•¨)
    
    plt.figure(figsize=(12, 10))
    
    # ì •ê·œí™”ëœ confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # ë¼ë²¨ ì„¤ì •
    class_labels = [LABEL_NAMES[i] for i in range(5)]
    
    # íˆíŠ¸ë§µ ìƒì„±
    sns.heatmap(cm_normalized, 
                annot=True, 
                fmt='.3f', 
                cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels,
                cbar_kws={'label': 'Normalized Frequency'},
                annot_kws={'size': 12})
    
    plt.title('Confusion Matrix', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(rotation=0, fontsize=12)
    
    # ì—¬ë°± ì¡°ì •
    plt.tight_layout()
    
    # ì €ì¥
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… Confusion Matrixê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

def plot_class_performance(results, save_path='class_performance.png'):
    """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”"""
    # ë™ì  ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í°íŠ¸ ì„¤ì •ë„ í¬í•¨)
 
    
    fig, axes = plt.subplots(2, 2, figsize=(18, 15))
    
    # ë¼ë²¨ ì„¤ì •
    class_names = [LABEL_NAMES[i] for i in range(5)]
    
    # 1. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    accuracies = [results['class_accuracies'][i] for i in range(5)]
    bars1 = axes[0, 0].bar(class_names, accuracies, color='skyblue', alpha=0.8, edgecolor='navy', linewidth=1)
    axes[0, 0].set_title('Class Accuracy', fontweight='bold', fontsize=16, pad=15)
    axes[0, 0].set_ylabel('Accuracy', fontsize=13, fontweight='bold')
    axes[0, 0].set_ylim(0, 1.1)
    axes[0, 0].tick_params(axis='x', rotation=35, labelsize=11)
    axes[0, 0].tick_params(axis='y', labelsize=11)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(accuracies):
        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=11, fontweight='bold')
    
    # 2. Precision, Recall, F1-score ë¹„êµ
    x = np.arange(len(class_names))
    width = 0.25
    
    bars2_1 = axes[0, 1].bar(x - width, results['precision'], width, label='Precision', alpha=0.8, color='lightcoral')
    bars2_2 = axes[0, 1].bar(x, results['recall'], width, label='Recall', alpha=0.8, color='lightgreen')
    bars2_3 = axes[0, 1].bar(x + width, results['f1_score'], width, label='F1-Score', alpha=0.8, color='lightsalmon')
    
    axes[0, 1].set_title('Precision, Recall, F1-Score', fontweight='bold', fontsize=16, pad=15)
    axes[0, 1].set_ylabel('Score', fontsize=13, fontweight='bold')
    axes[0, 1].set_ylim(0, 1.1)
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(class_names, rotation=35, ha='right', fontsize=11)
    axes[0, 1].tick_params(axis='y', labelsize=11)
    axes[0, 1].legend(fontsize=12, loc='upper right')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # 3. í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ (í›ˆë ¨ ë°ì´í„°)
    sample_counts = [results['class_total'].get(i, 0) for i in range(5)]
    bars3 = axes[1, 0].bar(class_names, sample_counts, color='lightcoral', alpha=0.8, edgecolor='darkred', linewidth=1)
    axes[1, 0].set_title('Total Sample Count', fontweight='bold', fontsize=16, pad=15)
    axes[1, 0].set_ylabel('Count', fontsize=13, fontweight='bold')
    axes[1, 0].tick_params(axis='x', rotation=35, labelsize=11)
    axes[1, 0].tick_params(axis='y', labelsize=11)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(sample_counts):
        if v > 0:
            axes[1, 0].text(i, v + max(sample_counts)*0.02, str(v), ha='center', fontsize=11, fontweight='bold')
    
    # 4. Support (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜)
    bars4 = axes[1, 1].bar(class_names, results['support'], color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=1)
    axes[1, 1].set_title('Test Set Sample Count', fontweight='bold', fontsize=16, pad=15)
    axes[1, 1].set_ylabel('Count', fontsize=13, fontweight='bold')
    axes[1, 1].tick_params(axis='x', rotation=35, labelsize=11)
    axes[1, 1].tick_params(axis='y', labelsize=11)
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(results['support']):
        if v > 0:
            axes[1, 1].text(i, v + max(results['support'])*0.02, str(v), ha='center', fontsize=11, fontweight='bold')
    
    # ì „ì²´ ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout(pad=3.0)
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… í´ë˜ìŠ¤ ì„±ëŠ¥ ì°¨íŠ¸ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

def analyze_misclassifications(results, top_k=10, save_misclassified=True):
    """ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„ ë° format ì €ì¥"""
    print(f"\nğŸ” ì˜¤ë¶„ë¥˜ ë¶„ì„ (ìƒìœ„ {top_k}ê°œ íŒ¨í„´)")
    print("-" * 60)
    
    # ì˜¤ë¶„ë¥˜ íŒ¨í„´ ìˆ˜ì§‘
    misclass_patterns = defaultdict(int)
    misclassified_data = defaultdict(list)  # ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´
    
    for idx, (true_label, pred_label, format_name) in enumerate(zip(results['labels'], results['predictions'], results['formats'])):
        if true_label != pred_label:
            pattern = f"{LABEL_NAMES[true_label]} â†’ {LABEL_NAMES[pred_label]}"
            misclass_patterns[pattern] += 1
            
            # ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´ ì €ì¥ (softmax ê°’ í¬í•¨)
            softmax_probs = results['logits'][idx]  # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ softmax í™•ë¥ 
            misclassified_data[pattern].append({
                'format': format_name,
                'true_label': true_label,
                'pred_label': pred_label,
                'true_label_name': LABEL_NAMES[true_label],
                'pred_label_name': LABEL_NAMES[pred_label],
                'softmax_probs': softmax_probs,
                'pred_confidence': softmax_probs[pred_label],  # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ì˜ í™•ë¥ 
                'true_confidence': softmax_probs[true_label],  # ì‹¤ì œ ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ 
                'confidence_diff': softmax_probs[pred_label] - softmax_probs[true_label]  # í™•ë¥  ì°¨ì´
            })
    
    # ìƒìœ„ íŒ¨í„´ ì¶œë ¥
    sorted_patterns = sorted(misclass_patterns.items(), key=lambda x: x[1], reverse=True)
    
    for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
        percentage = (count / len(results['predictions'])) * 100
        print(f"{i+1:2d}. {pattern:<50} : {count:4d}íšŒ ({percentage:.2f}%)")
    
    # ğŸ”¬ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„
    print(f"\nğŸ”¬ ìƒìœ„ {top_k}ê°œ íŒ¨í„´ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„")
    print("=" * 100)
    
    softmax_analysis = {}
    
    for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
        print(f"\n[íŒ¨í„´ {i+1}] {pattern} ({count}ê°œ ì¼€ì´ìŠ¤)")
        print("-" * 80)
        
        cases = misclassified_data[pattern]
        
        # í™•ë¥  í†µê³„ ê³„ì‚°
        pred_confidences = [case['pred_confidence'] for case in cases]
        true_confidences = [case['true_confidence'] for case in cases]
        confidence_diffs = [case['confidence_diff'] for case in cases]
        
        # í†µê³„ ì •ë³´
        stats = {
            'count': count,
            'pred_conf_mean': np.mean(pred_confidences),
            'pred_conf_std': np.std(pred_confidences),
            'pred_conf_min': np.min(pred_confidences),
            'pred_conf_max': np.max(pred_confidences),
            'true_conf_mean': np.mean(true_confidences),
            'true_conf_std': np.std(true_confidences),
            'true_conf_min': np.min(true_confidences),
            'true_conf_max': np.max(true_confidences),
            'diff_mean': np.mean(confidence_diffs),
            'diff_std': np.std(confidence_diffs),
            'high_confidence_wrong': sum(1 for conf in pred_confidences if conf > 0.7),  # ë†’ì€ í™•ì‹ ë„ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤
            'low_confidence_wrong': sum(1 for conf in pred_confidences if conf < 0.4),   # ë‚®ì€ í™•ì‹ ë„ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤
        }
        
        softmax_analysis[pattern] = stats
        
        print(f"ğŸ“Š ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:")
        print(f"   í‰ê· : {stats['pred_conf_mean']:.4f} Â± {stats['pred_conf_std']:.4f}")
        print(f"   ë²”ìœ„: {stats['pred_conf_min']:.4f} ~ {stats['pred_conf_max']:.4f}")
        
        print(f"ğŸ“Š ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:")
        print(f"   í‰ê· : {stats['true_conf_mean']:.4f} Â± {stats['true_conf_std']:.4f}")
        print(f"   ë²”ìœ„: {stats['true_conf_min']:.4f} ~ {stats['true_conf_max']:.4f}")
        
        print(f"ğŸ“Š í™•ë¥  ì°¨ì´ (ì˜ˆì¸¡ - ì‹¤ì œ):")
        print(f"   í‰ê· : {stats['diff_mean']:+.4f} Â± {stats['diff_std']:.4f}")
        
        print(f"ğŸ¯ í™•ì‹ ë„ ë¶„ì„:")
        print(f"   ë†’ì€ í™•ì‹ ë„(>0.7)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['high_confidence_wrong']}/{count} ({stats['high_confidence_wrong']/count*100:.1f}%)")
        print(f"   ë‚®ì€ í™•ì‹ ë„(<0.4)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['low_confidence_wrong']}/{count} ({stats['low_confidence_wrong']/count*100:.1f}%)")
        
        # í™•ë¥  ë¶„í¬ êµ¬ê°„ë³„ ë¶„ì„
        prob_bins = [(0.0, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 1.0)]
        print(f"ğŸ“ˆ ì˜ˆì¸¡ í™•ë¥  êµ¬ê°„ë³„ ë¶„í¬:")
        for low, high in prob_bins:
            count_in_bin = sum(1 for conf in pred_confidences if low <= conf < high)
            percentage_in_bin = count_in_bin / count * 100
            print(f"   {low:.1f}~{high:.1f}: {count_in_bin:3d}ê°œ ({percentage_in_bin:5.1f}%)")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    if save_misclassified:
        print(f"\nğŸ’¾ ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
        
        # Softmax ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open('misclassified_softmax_analysis.txt', 'w', encoding='utf-8') as f:
            f.write("ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(results['predictions'])}\n")
            f.write(f"ì´ ì˜¤ë¶„ë¥˜ ìˆ˜: {np.sum(results['labels'] != results['predictions'])}\n")
            f.write(f"ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f}\n\n")
            
            for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
                stats = softmax_analysis[pattern]
                f.write(f"\n{'='*60}\n")
                f.write(f"[íŒ¨í„´ {i+1}] {pattern} ({count}ê°œ ì¼€ì´ìŠ¤)\n")
                f.write(f"{'='*60}\n")
                
                f.write(f"\nğŸ“Š ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:\n")
                f.write(f"   í‰ê· : {stats['pred_conf_mean']:.4f} Â± {stats['pred_conf_std']:.4f}\n")
                f.write(f"   ë²”ìœ„: {stats['pred_conf_min']:.4f} ~ {stats['pred_conf_max']:.4f}\n")
                
                f.write(f"\nğŸ“Š ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:\n")
                f.write(f"   í‰ê· : {stats['true_conf_mean']:.4f} Â± {stats['true_conf_std']:.4f}\n")
                f.write(f"   ë²”ìœ„: {stats['true_conf_min']:.4f} ~ {stats['true_conf_max']:.4f}\n")
                
                f.write(f"\nğŸ“Š í™•ë¥  ì°¨ì´ (ì˜ˆì¸¡ - ì‹¤ì œ):\n")
                f.write(f"   í‰ê· : {stats['diff_mean']:+.4f} Â± {stats['diff_std']:.4f}\n")
                
                f.write(f"\nğŸ¯ í™•ì‹ ë„ ë¶„ì„:\n")
                f.write(f"   ë†’ì€ í™•ì‹ ë„(>0.7)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['high_confidence_wrong']}/{count} ({stats['high_confidence_wrong']/count*100:.1f}%)\n")
                f.write(f"   ë‚®ì€ í™•ì‹ ë„(<0.4)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['low_confidence_wrong']}/{count} ({stats['low_confidence_wrong']/count*100:.1f}%)\n")
                
                # ìƒì„¸ ì¼€ì´ìŠ¤ë³„ ì •ë³´
                f.write(f"\nğŸ“‹ ìƒì„¸ ì¼€ì´ìŠ¤ë³„ í™•ë¥  ì •ë³´:\n")
                f.write(f"{'No.':<4} {'Format':<30} {'ì˜ˆì¸¡í™•ë¥ ':<8} {'ì‹¤ì œí™•ë¥ ':<8} {'ì°¨ì´':<8}\n")
                f.write("-" * 60 + "\n")
                
                cases = misclassified_data[pattern]
                # ì˜ˆì¸¡ í™•ë¥ ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_cases = sorted(cases, key=lambda x: x['pred_confidence'], reverse=True)
                
                for j, case in enumerate(sorted_cases[:20]):  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                    f.write(f"{j+1:<4} {case['format']:<30} {case['pred_confidence']:<8.4f} {case['true_confidence']:<8.4f} {case['confidence_diff']:+8.4f}\n")
                
                if len(sorted_cases) > 20:
                    f.write(f"... (ì´ {len(sorted_cases)}ê°œ ì¤‘ ìƒìœ„ 20ê°œë§Œ í‘œì‹œ)\n")
        
      
        
     
        
        print(f"   âœ… misclassified_softmax_analysis.txt: Softmax í™•ë¥  ë¶„í¬ ìƒì„¸ ë¶„ì„")
   

# ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_comprehensive_evaluation(model, dataloader, device, save_plots=True):
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    print("ğŸš€ ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    results = evaluate_model_performance(model, dataloader, device)
    
    # ë³´ê³ ì„œ ì¶œë ¥
    print_performance_report(results)
    
    # ì˜¤ë¶„ë¥˜ ë¶„ì„
    analyze_misclassifications(results)
    
    if save_plots:
        # ì‹œê°í™”
        plot_confusion_matrix(results['confusion_matrix'])
        plot_class_performance(results)
    
    # ìš”ì•½ í†µê³„
    print(f"\nğŸ“Œ ìš”ì•½ í†µê³„:")
    print(f"   â€¢ ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f}")
    print(f"   â€¢ í‰ê·  F1-ì ìˆ˜: {np.mean(results['f1_score']):.4f}")
    print(f"   â€¢ ê°€ì¤‘ F1-ì ìˆ˜: {np.average(results['f1_score'], weights=results['support']):.4f}")
    print(f"   â€¢ ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(results['predictions'])}")
    print(f"   â€¢ ì´ ì˜¤ë¶„ë¥˜: {np.sum(results['labels'] != results['predictions'])}")
    
    return results

# ê¸°ì¡´ ì‹¤í—˜ ì½”ë“œë¥¼ í™•ì¥
def run_basic_test(model, dataloader, device):
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì½”ë“œ)"""
    print("ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    
    for batch in dataloader:
        landmarks = batch['landmarks'].to(device)
        label = batch['label'].long().to(device)
        logits = model(landmarks)
        _, predicted = torch.max(logits.data, 1)
        print("ì˜ˆì¸¡ê°’:", predicted)
        print("ì‹¤ì œê°’:", label)
        break

def test_experiment():
    model = PointNetClassifier(num_classes=5)
    
    # ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Validation ë°ì´í„°ì…‹ ë¡œë“œ (ì´ë¯¸ ê²€ì¦ìš© ë°ì´í„°)
    test_dataset = FocusDataset_multi(base_path)
    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    print(f"Validation ë°ì´í„°ì…‹ í¬ê¸°: {len(test_dataset)}")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì½”ë“œ)
    print("\n" + "="*50)
    print("1. ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
    print("="*50)
    run_basic_test(model, test_dataloader, device)
    
    # ì¢…í•©ì ì¸ ì„±ëŠ¥ í‰ê°€
    print("\n" + "="*50)  
    print("2. ì¢…í•© ì„±ëŠ¥ í‰ê°€")
    print("="*50)
    results = run_comprehensive_evaluation(model, test_dataloader, device, save_plots=True)
    
    # ì¶”ê°€ ë¶„ì„ (ì„ íƒì‚¬í•­)
    print("\n" + "="*50)
    print("3. ì¶”ê°€ ë¶„ì„")
    print("="*50)
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
    class_distribution = results['class_total']
    total_samples = sum(class_distribution.values())
    
    print("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„:")
    for i in range(5):
        count = class_distribution.get(i, 0)
        percentage = (count / total_samples) * 100
        print(f"   {LABEL_NAMES[i]}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€/ë‚˜ìœ í´ë˜ìŠ¤
    f1_scores = results['f1_score']
    best_class = np.argmax(f1_scores)
    worst_class = np.argmin(f1_scores)
    
    print(f"\nğŸ† ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ í´ë˜ìŠ¤: {LABEL_NAMES[best_class]} (F1: {f1_scores[best_class]:.4f})")
    print(f"ğŸš¨ ì„±ëŠ¥ì´ ê°€ì¥ ë‚˜ìœ í´ë˜ìŠ¤: {LABEL_NAMES[worst_class]} (F1: {f1_scores[worst_class]:.4f})")
    
    return results

if __name__ == "__main__":
    print("ğŸš€ FocusSu AI ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # matplotlib ë°±ì—”ë“œ ì„¤ì •
    import matplotlib

    
    # ì‹¤í—˜ ì‹¤í–‰
    print("\n" + "="*60)
    results = test_experiment()
    
    print("\n" + "="*80)
    print("ğŸ‰ ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ìƒì„±ëœ íŒŒì¼:")
    print("  ğŸ“Š ì‹œê°í™”:")
    print("    - confusion_matrix.png: í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ")
    print("    - class_performance.png: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ì°¨íŠ¸")
    print("  ğŸ“„ ì˜¤ë¶„ë¥˜ ë¶„ì„:")
    print("    - misclassified_softmax_analysis.txt: Softmax í™•ë¥  ë¶„í¬ ìƒì„¸ ë¶„ì„")
    print("="*80)