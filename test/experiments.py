import torch
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from collections import defaultdict
import pandas as pd
from torch.utils.data import DataLoader
from model.PointNet import PointNetClassifier
from train.data import FocusDataset_multi
import os
from sklearn.decomposition import PCA


import matplotlib.font_manager
font_list = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
[matplotlib.font_manager.FontProperties(fname=font).get_name() for font in font_list if 'Nanum' in font]
\
import matplotlib.pyplot as plt
plt.rc('font', family='NanumGothicCoding')
\
import matplotlib as mpl
mpl.rcParams['axes.unicode_minus'] = False


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
base_path = '/shared_data/focussu/109.í•™ìŠµíƒœë„_ë°_ì„±í–¥_ê´€ì°°_ë°ì´í„°/3.ê°œë°©ë°ì´í„°/1.ë°ì´í„°/Validation'
MODEL_PATH = 'model/best_multi_model.pth'

# experiments í´ë” ìƒì„±
EXPERIMENTS_DIR = 'experiments'
os.makedirs(EXPERIMENTS_DIR, exist_ok=True)

# ë¼ë²¨ ì •ì˜
LABEL_NAMES = {
    0: "ì§‘ì¤‘(í¥ë¯¸ë¡œì›€)",
    1: "ì§‘ì¤‘(ì°¨ë¶„í•¨)", 
    2: "ë¹„ì§‘ì¤‘(ì°¨ë¶„í•¨)",
    3: "ë¹„ì§‘ì¤‘(ì§€ë£¨í•¨)",
    4: "ì¡¸ìŒ"
}

def evaluate_model_performance(model, dataloader, device):
    """ëª¨ë¸ì˜ ì¢…í•©ì ì¸ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ë³´ê³ ì„œ ìƒì„±"""
    model.eval()
    
    all_predictions = []
    all_labels = []
    all_logits = []  # softmax í™•ë¥ 
    all_raw_logits = []  # raw logits (softmax ì´ì „)
    all_features = []  # ëª¨ë¸ì˜ feature ì¶”ì¶œ ë¶€ë¶„
    all_formats = []  # format ì •ë³´ ì €ì¥
    class_correct = defaultdict(int)
    class_total = defaultdict(int)
    
    print("ëª¨ë¸ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            landmarks = batch['landmarks'].to(device)
            labels = batch['label'].long().to(device)
            formats = batch['format']  # format ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            
            # ì˜ˆì¸¡ ìˆ˜í–‰ (featureì™€ raw logits ëª¨ë‘ ì¶”ì¶œ)
            logits = model(landmarks)
            
            # ëª¨ë¸ì˜ feature extraction ë¶€ë¶„ë§Œ ì‹¤í–‰ (PointNetì˜ ê²½ìš°)
            try:
                # PointNetì˜ feature extraction ë¶€ë¶„ ì‹¤í–‰
                features = model.extract_features(landmarks)
                all_features.extend(features.cpu().numpy())
            except AttributeError:
                # extract_features ë©”ì†Œë“œê°€ ì—†ëŠ” ê²½ìš°, raw logits ì‚¬ìš©
                all_features.extend(logits.cpu().numpy())  # raw logitsë¥¼ featuresë¡œ ì‚¬ìš©
            
            _, predicted = torch.max(logits.data, 1)
            
            # ê²°ê³¼ ì €ì¥
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_raw_logits.extend(logits.cpu().numpy())  # raw logits ì €ì¥
            all_logits.extend(torch.softmax(logits, dim=1).cpu().numpy())  # softmax í™•ë¥ 
            all_formats.extend(formats)  # format ì •ë³´ ì €ì¥
            
            # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘
            for i in range(labels.size(0)):
                label = labels[i].item()
                class_total[label] += 1
                if predicted[i] == labels[i]:
                    class_correct[label] += 1
            
            if batch_idx % 10 == 0:
                print(f"ë°°ì¹˜ {batch_idx}/{len(dataloader)} ì²˜ë¦¬ ì™„ë£Œ")
    
    # NumPy ë°°ì—´ë¡œ ë³€í™˜
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_logits = np.array(all_logits)
    all_raw_logits = np.array(all_raw_logits)
    all_features = np.array(all_features)
    
    print(f"\nì´ {len(all_predictions)}ê°œ ìƒ˜í”Œ í‰ê°€ ì™„ë£Œ")
    print(f"Feature ì°¨ì›: {all_features.shape}")
    print(f"Raw logits ì°¨ì›: {all_raw_logits.shape}")
    print(f"Softmax í™•ë¥  ì°¨ì›: {all_logits.shape}")
    
    # 1. ì „ì²´ ì •í™•ë„
    overall_accuracy = accuracy_score(all_labels, all_predictions)
    
    # 2. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    class_accuracies = {}
    for class_id in range(5):
        if class_total[class_id] > 0:
            class_accuracies[class_id] = class_correct[class_id] / class_total[class_id]
        else:
            class_accuracies[class_id] = 0.0
    
    # 3. Precision, Recall, F1-score
    precision, recall, f1, support = precision_recall_fscore_support(
        all_labels, all_predictions, average=None, zero_division=0
    )
    
    # 4. Confusion Matrix
    cm = confusion_matrix(all_labels, all_predictions)
    
    # 5. Classification Report
    class_report = classification_report(
        all_labels, all_predictions, 
        target_names=[LABEL_NAMES[i] for i in range(5)],
        zero_division=0
    )
    
    return {
        'overall_accuracy': overall_accuracy,
        'class_accuracies': class_accuracies,
        'class_total': dict(class_total),
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'support': support,
        'confusion_matrix': cm,
        'classification_report': class_report,
        'predictions': all_predictions,
        'labels': all_labels,
        'logits': all_logits,  # softmax í™•ë¥ 
        'raw_logits': all_raw_logits,  # raw logits
        'features': all_features,  # feature vectors
        'formats': all_formats  # format ì •ë³´ ì¶”ê°€
    }

def print_performance_report(results):
    """ì„±ëŠ¥ ì§€í‘œ ë³´ê³ ì„œë¥¼ ì¶œë ¥"""
    print("=" * 80)
    print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë³´ê³ ì„œ")
    print("=" * 80)
    
    # ì „ì²´ ì •í™•ë„
    print(f"\nğŸ“Š ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f} ({results['overall_accuracy']*100:.2f}%)")
    
    
    # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì •ë³´
    print(f"\nğŸ“‹ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ:")
    print("-" * 80)
    print(f"{'í´ë˜ìŠ¤':<15} {'ìƒ˜í”Œìˆ˜':<8} {'ì •í™•ë„':<8} {'ì •ë°€ë„':<8} {'ì¬í˜„ìœ¨':<8} {'F1ì ìˆ˜':<8}")
    print("-" * 80)
    
    for i in range(5):
        class_name = LABEL_NAMES[i]
        sample_count = results['class_total'].get(i, 0)
        accuracy = results['class_accuracies'][i]
        precision = results['precision'][i]
        recall = results['recall'][i]
        f1 = results['f1_score'][i]
        
        print(f"{class_name:<15} {sample_count:<8} {accuracy:<8.4f} {precision:<8.4f} {recall:<8.4f} {f1:<8.4f}")
    
    # í‰ê·  ì§€í‘œ
    print("-" * 80)
    avg_precision = np.mean(results['precision'])
    avg_recall = np.mean(results['recall'])
    avg_f1 = np.mean(results['f1_score'])
    
    print(f"{'í‰ê· ':<15} {'':<8} {'':<8} {avg_precision:<8.4f} {avg_recall:<8.4f} {avg_f1:<8.4f}")
    
    # ê°€ì¤‘ í‰ê·  ì§€í‘œ
    weighted_precision = np.average(results['precision'], weights=results['support'])
    weighted_recall = np.average(results['recall'], weights=results['support'])
    weighted_f1 = np.average(results['f1_score'], weights=results['support'])
    
    print(f"{'ê°€ì¤‘í‰ê· ':<15} {'':<8} {'':<8} {weighted_precision:<8.4f} {weighted_recall:<8.4f} {weighted_f1:<8.4f}")
    
    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
    print(f"\nğŸ“ˆ ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ:")
    print(results['classification_report'])

def plot_confusion_matrix(cm, save_path=None):
    """Confusion Matrix ì‹œê°í™”"""
    if save_path is None:
        save_path = os.path.join(EXPERIMENTS_DIR, 'confusion_matrix.png')
    
    # ë™ì  ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í°íŠ¸ ì„¤ì •ë„ í¬í•¨)
    
    plt.figure(figsize=(12, 10))
    
    # ì •ê·œí™”ëœ confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # ë¼ë²¨ ì„¤ì •
    class_labels = [LABEL_NAMES[i] for i in range(5)]
    
    # íˆíŠ¸ë§µ ìƒì„±
    sns.heatmap(cm_normalized, 
                annot=True, 
                fmt='.3f', 
                cmap='Blues',
                xticklabels=class_labels,
                yticklabels=class_labels,
                cbar_kws={'label': 'Normalized Frequency'},
                annot_kws={'size': 12})
    
    plt.title('Confusion Matrix', fontsize=18, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(rotation=0, fontsize=12)
    
    # ì—¬ë°± ì¡°ì •
    plt.tight_layout()
    
    # ì €ì¥
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… Confusion Matrixê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

def plot_class_performance(results, save_path=None):
    """í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”"""
    if save_path is None:
        save_path = os.path.join(EXPERIMENTS_DIR, 'class_performance.png')
    
    # ë™ì  ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í°íŠ¸ ì„¤ì •ë„ í¬í•¨)
 
    
    fig, axes = plt.subplots(2, 2, figsize=(18, 15))
    
    # ë¼ë²¨ ì„¤ì •
    class_names = [LABEL_NAMES[i] for i in range(5)]
    
    # 1. í´ë˜ìŠ¤ë³„ ì •í™•ë„
    accuracies = [results['class_accuracies'][i] for i in range(5)]
    bars1 = axes[0, 0].bar(class_names, accuracies, color='skyblue', alpha=0.8, edgecolor='navy', linewidth=1)
    axes[0, 0].set_title('Class Accuracy', fontweight='bold', fontsize=16, pad=15)
    axes[0, 0].set_ylabel('Accuracy', fontsize=13, fontweight='bold')
    axes[0, 0].set_ylim(0, 1.1)
    axes[0, 0].tick_params(axis='x', rotation=35, labelsize=11)
    axes[0, 0].tick_params(axis='y', labelsize=11)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(accuracies):
        axes[0, 0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=11, fontweight='bold')
    
    # 2. Precision, Recall, F1-score ë¹„êµ
    x = np.arange(len(class_names))
    width = 0.25
    
    bars2_1 = axes[0, 1].bar(x - width, results['precision'], width, label='Precision', alpha=0.8, color='lightcoral')
    bars2_2 = axes[0, 1].bar(x, results['recall'], width, label='Recall', alpha=0.8, color='lightgreen')
    bars2_3 = axes[0, 1].bar(x + width, results['f1_score'], width, label='F1-Score', alpha=0.8, color='lightsalmon')
    
    axes[0, 1].set_title('Precision, Recall, F1-Score', fontweight='bold', fontsize=16, pad=15)
    axes[0, 1].set_ylabel('Score', fontsize=13, fontweight='bold')
    axes[0, 1].set_ylim(0, 1.1)
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(class_names, rotation=35, ha='right', fontsize=11)
    axes[0, 1].tick_params(axis='y', labelsize=11)
    axes[0, 1].legend(fontsize=12, loc='upper right')
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # 3. í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ (í›ˆë ¨ ë°ì´í„°)
    sample_counts = [results['class_total'].get(i, 0) for i in range(5)]
    bars3 = axes[1, 0].bar(class_names, sample_counts, color='lightcoral', alpha=0.8, edgecolor='darkred', linewidth=1)
    axes[1, 0].set_title('Total Sample Count', fontweight='bold', fontsize=16, pad=15)
    axes[1, 0].set_ylabel('Count', fontsize=13, fontweight='bold')
    axes[1, 0].tick_params(axis='x', rotation=35, labelsize=11)
    axes[1, 0].tick_params(axis='y', labelsize=11)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(sample_counts):
        if v > 0:
            axes[1, 0].text(i, v + max(sample_counts)*0.02, str(v), ha='center', fontsize=11, fontweight='bold')
    
    # 4. Support (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜)
    bars4 = axes[1, 1].bar(class_names, results['support'], color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=1)
    axes[1, 1].set_title('Test Set Sample Count', fontweight='bold', fontsize=16, pad=15)
    axes[1, 1].set_ylabel('Count', fontsize=13, fontweight='bold')
    axes[1, 1].tick_params(axis='x', rotation=35, labelsize=11)
    axes[1, 1].tick_params(axis='y', labelsize=11)
    axes[1, 1].grid(True, alpha=0.3, axis='y')
    
    # ê°’ í‘œì‹œ
    for i, v in enumerate(results['support']):
        if v > 0:
            axes[1, 1].text(i, v + max(results['support'])*0.02, str(v), ha='center', fontsize=11, fontweight='bold')
    
    # ì „ì²´ ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout(pad=3.0)
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… í´ë˜ìŠ¤ ì„±ëŠ¥ ì°¨íŠ¸ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

def plot_tsne_visualization(results, perplexity=30, n_iter=1000, save_path=None, 
                           input_type='raw_logits', max_samples=5000):
    """t-SNEë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ ê²°ê³¼ ë¶„í¬ ì‹œê°í™”
    
    Args:
        input_type: 't-SNE ì…ë ¥ ë°ì´í„° ìœ í˜•
            - 'features': ëª¨ë¸ì˜ feature vectors (ê¶Œì¥)
            - 'raw_logits': raw logits (softmax ì´ì „)
            - 'softmax': softmax í™•ë¥  (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
    """
    if save_path is None:
        save_path = os.path.join(EXPERIMENTS_DIR, f'tsne_{input_type}_visualization.png')
    
    # ì…ë ¥ ë°ì´í„° ì„ íƒ
    if input_type == 'features':
        input_data = results['features']
        data_name = "Feature Vectors"
    elif input_type == 'raw_logits':
        input_data = results['raw_logits']
        data_name = "Raw Logits"
    elif input_type == 'softmax':
        input_data = results['logits']
        data_name = "Softmax Probabilities"
    else:
        raise ValueError("input_typeì€ 'features', 'raw_logits', 'softmax' ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    
    true_labels = results['labels']
    predicted_labels = results['predictions']
    
    print(f"\nğŸ” t-SNE ë¶„ì„ ({data_name} ì‚¬ìš©)")
    print(f"   â€¢ ì…ë ¥ ë°ì´í„° í˜•íƒœ: {input_data.shape}")
    print(f"   â€¢ ì…ë ¥ ë°ì´í„° íƒ€ì…: {input_type}")
    
    # ë°ì´í„° íŠ¹ì„± ë¶„ì„
    print(f"   â€¢ ë°ì´í„° ë²”ìœ„: [{input_data.min():.4f}, {input_data.max():.4f}]")
    print(f"   â€¢ ë°ì´í„° í‰ê· : {input_data.mean():.4f}")
    print(f"   â€¢ ë°ì´í„° í‘œì¤€í¸ì°¨: {input_data.std():.4f}")
    
    # Softmax í™•ë¥  ì‚¬ìš© ì‹œ ê²½ê³ 
    if input_type == 'softmax':
        print("âš ï¸  ê²½ê³ : Softmax í™•ë¥ ì„ t-SNE ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        print("   - SoftmaxëŠ” simplex ì œì•½ìœ¼ë¡œ ì¸í•´ ì •ë³´ê°€ ì œí•œë©ë‹ˆë‹¤.")
        print("   - Raw logits ë˜ëŠ” feature vectors ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ë°ì´í„° ìƒ˜í”Œë§
    if len(input_data) > max_samples:
        print(f"âš ï¸  ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ {len(input_data)}ê°œ ìƒ˜í”Œ ì¤‘ {max_samples}ê°œë¥¼ ëœë¤ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.")
        
        # í´ë˜ìŠ¤ë³„ ê· ë“± ìƒ˜í”Œë§
        sampled_indices = []
        samples_per_class = max_samples // 5
        
        for class_id in range(5):
            class_indices = np.where(true_labels == class_id)[0]
            if len(class_indices) > samples_per_class:
                selected = np.random.choice(class_indices, samples_per_class, replace=False)
            else:
                selected = class_indices
            sampled_indices.extend(selected)
        
        # ë‚¨ì€ ìƒ˜í”Œë¡œ ë¶€ì¡±í•œ ë¶€ë¶„ ì±„ìš°ê¸°
        if len(sampled_indices) < max_samples:
            remaining_indices = np.setdiff1d(np.arange(len(input_data)), sampled_indices)
            additional_needed = max_samples - len(sampled_indices)
            if len(remaining_indices) > 0:
                additional = np.random.choice(remaining_indices, 
                                            min(additional_needed, len(remaining_indices)), 
                                            replace=False)
                sampled_indices.extend(additional)
        
        sampled_indices = np.array(sampled_indices)
        
        # ìƒ˜í”Œë§ëœ ë°ì´í„° ì‚¬ìš©
        input_data = input_data[sampled_indices]
        true_labels = true_labels[sampled_indices]
        predicted_labels = predicted_labels[sampled_indices]
        
        print(f"   â€¢ ìƒ˜í”Œë§ í›„ ë°ì´í„° í˜•íƒœ: {input_data.shape}")
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ í™•ì¸
        print("   â€¢ í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜:")
        for i in range(5):
            count = np.sum(true_labels == i)
            print(f"     - {LABEL_NAMES[i]}: {count}ê°œ")
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ t-SNE íŒŒë¼ë¯¸í„° ì¡°ì •
    if perplexity >= len(input_data) / 3:
        perplexity = max(5, len(input_data) // 4)
        print(f"âš ï¸  Perplexityë¥¼ {perplexity}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.")
    
    try:
        print(f"ğŸ” t-SNE ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        print(f"   â€¢ Perplexity: {perplexity}")
        print(f"   â€¢ ë°˜ë³µ íšŸìˆ˜: {n_iter}")
        print(f"   â€¢ ìƒ˜í”Œ ìˆ˜: {len(input_data)}")
        print("   â€¢ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ t-SNE ì„¤ì •
        tsne = TSNE(
            n_components=2, 
            perplexity=perplexity, 
            n_iter=n_iter, 
            random_state=405, 
            verbose=0,
            n_jobs=1,
            learning_rate='auto'
        )
        tsne_results = tsne.fit_transform(input_data)
        
        print("âœ… t-SNE ì™„ë£Œ!")
        
    except MemoryError:
        print("âŒ t-SNE ì‹¤í–‰ ì¤‘ ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ”„ PCAë¡œ ëŒ€ì²´í•˜ì—¬ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
        
        pca = PCA(n_components=2, random_state=405)
        tsne_results = pca.fit_transform(input_data)
        
        print(f"âœ… PCA ì°¨ì› ì¶•ì†Œ ì™„ë£Œ (ì„¤ëª… ë¶„ì‚°ë¹„: {pca.explained_variance_ratio_.sum():.3f})")
        save_path = save_path.replace('tsne_', 'pca_')
        data_name = f"PCA ({data_name})"
    
    except Exception as e:
        print(f"âŒ ì°¨ì› ì¶•ì†Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ”„ ë‹¨ìˆœ PCAë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
        
        pca = PCA(n_components=2, random_state=42)
        tsne_results = pca.fit_transform(input_data)
        save_path = save_path.replace('tsne_', 'pca_')
        data_name = f"PCA ({data_name})"
    
    # ì‹œê°í™” - ë” ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ìƒ‰ìƒ ì‚¬ìš©
    fig, axes = plt.subplots(1, 3, figsize=(24, 8))
    
    # ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” 5ê°€ì§€ ìƒ‰ìƒ (ìƒ‰ë§¹ ì¹œí™”ì )
    colors = ['#D32F2F', '#1976D2', '#388E3C', '#7B1FA2', '#F57C00']  # ë¹¨ê°•, íŒŒë‘, ì´ˆë¡, ë³´ë¼, ì£¼í™©
    class_names = [LABEL_NAMES[i] for i in range(5)]
    
    method_name = 'PCA' if 'pca_' in save_path else 't-SNE'
    
    # 1. ì‹¤ì œ ë¼ë²¨ë³„ ë¶„í¬
    for i in range(5):
        mask = true_labels == i
        if np.sum(mask) > 0:
            axes[0].scatter(tsne_results[mask, 0], tsne_results[mask, 1], 
                           c=colors[i], label=class_names[i], alpha=0.8, s=25, edgecolors='black', linewidth=0.5)
    
    axes[0].set_title(f'{method_name}: ì‹¤ì œ ë¼ë²¨ë³„ ë¶„í¬\n({data_name})', 
                      fontsize=16, fontweight='bold', pad=15)
    axes[0].set_xlabel(f'{method_name} ì°¨ì› 1', fontsize=12)
    axes[0].set_ylabel(f'{method_name} ì°¨ì› 2', fontsize=12)
    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    axes[0].grid(True, alpha=0.3)
    
    # 2. ì˜ˆì¸¡ ë¼ë²¨ë³„ ë¶„í¬
    for i in range(5):
        mask = predicted_labels == i
        if np.sum(mask) > 0:
            axes[1].scatter(tsne_results[mask, 0], tsne_results[mask, 1], 
                           c=colors[i], label=class_names[i], alpha=0.8, s=25, edgecolors='black', linewidth=0.5)
    
    axes[1].set_title(f'{method_name}: ì˜ˆì¸¡ ë¼ë²¨ë³„ ë¶„í¬\n({data_name})', 
                      fontsize=16, fontweight='bold', pad=15)
    axes[1].set_xlabel(f'{method_name} ì°¨ì› 1', fontsize=12)
    axes[1].set_ylabel(f'{method_name} ì°¨ì› 2', fontsize=12)
    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    axes[1].grid(True, alpha=0.3)
    
    # 3. ì •ë¶„ë¥˜/ì˜¤ë¶„ë¥˜ êµ¬ë¶„
    correct_mask = true_labels == predicted_labels
    incorrect_mask = ~correct_mask
    
    if np.sum(correct_mask) > 0:
        axes[2].scatter(tsne_results[correct_mask, 0], tsne_results[correct_mask, 1], 
                       c='#2E7D32', label=f'ì •ë¶„ë¥˜ ({np.sum(correct_mask)}ê°œ)', 
                       alpha=0.8, s=25, edgecolors='black', linewidth=0.5)
    
    if np.sum(incorrect_mask) > 0:
        axes[2].scatter(tsne_results[incorrect_mask, 0], tsne_results[incorrect_mask, 1], 
                       c='#C62828', label=f'ì˜¤ë¶„ë¥˜ ({np.sum(incorrect_mask)}ê°œ)', 
                       alpha=0.8, s=25, edgecolors='black', linewidth=0.5)
    
    axes[2].set_title(f'{method_name}: ì •ë¶„ë¥˜ vs ì˜¤ë¶„ë¥˜\n({data_name})', 
                      fontsize=16, fontweight='bold', pad=15)
    axes[2].set_xlabel(f'{method_name} ì°¨ì› 1', fontsize=12)
    axes[2].set_ylabel(f'{method_name} ì°¨ì› 2', fontsize=12)
    axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… {method_name} ì‹œê°í™”ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()
    
    # í†µê³„ ì •ë³´ ì¶œë ¥
    print(f"\nğŸ“Š {method_name} ì‹œê°í™” í†µê³„:")
    print(f"   â€¢ ì…ë ¥ ë°ì´í„°: {data_name}")
    print(f"   â€¢ ì‚¬ìš©ëœ ìƒ˜í”Œ ìˆ˜: {len(tsne_results)}")
    print(f"   â€¢ ì •ë¶„ë¥˜ ìƒ˜í”Œ: {np.sum(correct_mask)} ({np.sum(correct_mask)/len(tsne_results)*100:.1f}%)")
    print(f"   â€¢ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ: {np.sum(incorrect_mask)} ({np.sum(incorrect_mask)/len(tsne_results)*100:.1f}%)")
    if method_name == 't-SNE':
        print(f"   â€¢ Perplexity: {perplexity}")
        print(f"   â€¢ ë°˜ë³µ íšŸìˆ˜: {n_iter}")
    
    # í´ë˜ìŠ¤ë³„ í´ëŸ¬ìŠ¤í„° ë°€ì§‘ë„ ë¶„ì„
    print(f"\nğŸ” í´ë˜ìŠ¤ë³„ í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
    for i in range(5):
        class_mask = true_labels == i
        class_count = np.sum(class_mask)
        if class_count > 1:
            class_points = tsne_results[class_mask]
            
            # ìƒ˜í”Œì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ì—¬ ê±°ë¦¬ ê³„ì‚°
            if len(class_points) > 100:
                sample_indices = np.random.choice(len(class_points), 100, replace=False)
                class_points_sample = class_points[sample_indices]
            else:
                class_points_sample = class_points
            
            # í´ë˜ìŠ¤ ë‚´ ì ë“¤ ê°„ì˜ í‰ê·  ê±°ë¦¬ ê³„ì‚°
            if len(class_points_sample) > 1:
                from scipy.spatial.distance import pdist
                distances = pdist(class_points_sample)
                avg_distance = np.mean(distances)
                std_distance = np.std(distances)
                print(f"   â€¢ {class_names[i]} ({class_count}ê°œ): í‰ê·  ê±°ë¦¬ {avg_distance:.2f} Â± {std_distance:.2f}")
            else:
                print(f"   â€¢ {class_names[i]} ({class_count}ê°œ): ê±°ë¦¬ ê³„ì‚° ë¶ˆê°€ (ìƒ˜í”Œ ë¶€ì¡±)")
        else:
            print(f"   â€¢ {class_names[i]} ({class_count}ê°œ): ë¶„ì„ ë¶ˆê°€ (ìƒ˜í”Œ ë¶€ì¡±)")

def compare_tsne_inputs(results, save_path=None):
    """ë‹¤ì–‘í•œ ì…ë ¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ t-SNE ë¹„êµ ì‹œê°í™”"""
    if save_path is None:
        save_path = os.path.join(EXPERIMENTS_DIR, 'tsne_comparison.png')
    
    print("\nğŸ” ë‹¤ì–‘í•œ ì…ë ¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ t-SNE ë¹„êµ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ì‚¬ìš©í•  ì…ë ¥ ë°ì´í„°ë“¤
    input_types = ['features', 'raw_logits', 'softmax']
    input_names = ['Feature Vectors', 'Raw Logits', 'Softmax Probabilities']
    
    fig, axes = plt.subplots(1, 3, figsize=(24, 8))
    
    # ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” 5ê°€ì§€ ìƒ‰ìƒ
    colors = ['#D32F2F', '#1976D2', '#388E3C', '#7B1FA2', '#F57C00']  # ë¹¨ê°•, íŒŒë‘, ì´ˆë¡, ë³´ë¼, ì£¼í™©
    class_names = [LABEL_NAMES[i] for i in range(5)]
    
    for idx, (input_type, input_name) in enumerate(zip(input_types, input_names)):
        print(f"\nğŸ“Š {input_name} ë¶„ì„ ì¤‘...")
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        if input_type == 'features':
            input_data = results['features']
        elif input_type == 'raw_logits':
            input_data = results['raw_logits']
        else:  # softmax
            input_data = results['logits']
        
        true_labels = results['labels']
        
        # ìƒ˜í”Œë§ (ë©”ëª¨ë¦¬ ì ˆì•½)
        max_samples = 2000  # ë¹„êµë¥¼ ìœ„í•´ ë” ì ì€ ìƒ˜í”Œ ì‚¬ìš©
        if len(input_data) > max_samples:
            indices = np.random.choice(len(input_data), max_samples, replace=False)
            input_data = input_data[indices]
            true_labels = true_labels[indices]
        
        try:
            # t-SNE ìˆ˜í–‰
            tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, 
                       random_state=405, verbose=0)
            tsne_results = tsne.fit_transform(input_data)
            
            # ì‹œê°í™”
            for i in range(5):
                mask = true_labels == i
                if np.sum(mask) > 0:
                    axes[idx].scatter(tsne_results[mask, 0], tsne_results[mask, 1], 
                                     c=colors[i], label=class_names[i], alpha=0.8, s=20, 
                                     edgecolors='black', linewidth=0.3)
            
            axes[idx].set_title(f't-SNE: {input_name}\n'
                               f'({input_data.shape[0]} samples, {input_data.shape[1]} dims)', 
                               fontsize=14, fontweight='bold')
            axes[idx].set_xlabel('t-SNE ì°¨ì› 1', fontsize=12)
            axes[idx].set_ylabel('t-SNE ì°¨ì› 2', fontsize=12)
            if idx == 0:  # ì²« ë²ˆì§¸ í”Œë¡¯ì—ë§Œ ë²”ë¡€ í‘œì‹œ
                axes[idx].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
            axes[idx].grid(True, alpha=0.3)
            
        except Exception as e:
            print(f"âŒ {input_name} t-SNE ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            axes[idx].text(0.5, 0.5, f'ì˜¤ë¥˜ ë°œìƒ\n{input_name}', 
                          ha='center', va='center', transform=axes[idx].transAxes)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… t-SNE ë¹„êµ ì‹œê°í™”ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()

def plot_confidence_distribution(results, save_path=None):
    """ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬ ì‹œê°í™”"""
    if save_path is None:
        save_path = os.path.join(EXPERIMENTS_DIR, 'confidence_distribution.png')
    
    print("ğŸ¯ ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤...")
    
    # ê° ì˜ˆì¸¡ì— ëŒ€í•œ ìµœëŒ€ í™•ë¥  (í™•ì‹ ë„) ê³„ì‚°
    max_probs = np.max(results['logits'], axis=1)
    true_labels = results['labels']
    predicted_labels = results['predictions']
    correct_mask = true_labels == predicted_labels
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. ì „ì²´ í™•ì‹ ë„ ë¶„í¬
    axes[0, 0].hist(max_probs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].axvline(np.mean(max_probs), color='red', linestyle='--', 
                       label=f'í‰ê· : {np.mean(max_probs):.3f}')
    axes[0, 0].set_title('ì „ì²´ ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('í™•ì‹ ë„ (ìµœëŒ€ í™•ë¥ )', fontsize=12)
    axes[0, 0].set_ylabel('ë¹ˆë„', fontsize=12)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ì •ë¶„ë¥˜ vs ì˜¤ë¶„ë¥˜ í™•ì‹ ë„ ë¹„êµ
    correct_probs = max_probs[correct_mask]
    incorrect_probs = max_probs[~correct_mask]
    
    axes[0, 1].hist(correct_probs, bins=20, alpha=0.7, color='green', 
                    label=f'ì •ë¶„ë¥˜ (í‰ê· : {np.mean(correct_probs):.3f})', density=True)
    axes[0, 1].hist(incorrect_probs, bins=20, alpha=0.7, color='red', 
                    label=f'ì˜¤ë¶„ë¥˜ (í‰ê· : {np.mean(incorrect_probs):.3f})', density=True)
    axes[0, 1].set_title('ì •ë¶„ë¥˜ vs ì˜¤ë¶„ë¥˜ í™•ì‹ ë„ ë¶„í¬', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('í™•ì‹ ë„ (ìµœëŒ€ í™•ë¥ )', fontsize=12)
    axes[0, 1].set_ylabel('ë°€ë„', fontsize=12)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. í´ë˜ìŠ¤ë³„ í™•ì‹ ë„ ë¶„í¬
    class_names = [LABEL_NAMES[i] for i in range(5)]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
    
    for i in range(5):
        class_mask = predicted_labels == i
        if np.sum(class_mask) > 0:
            class_probs = max_probs[class_mask]
            axes[1, 0].hist(class_probs, bins=15, alpha=0.6, color=colors[i], 
                           label=f'{class_names[i]} (í‰ê· : {np.mean(class_probs):.3f})', density=True)
    
    axes[1, 0].set_title('í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬', fontsize=14, fontweight='bold')
    axes[1, 0].set_xlabel('í™•ì‹ ë„ (ìµœëŒ€ í™•ë¥ )', fontsize=12)
    axes[1, 0].set_ylabel('ë°€ë„', fontsize=12)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. í™•ì‹ ë„ ì„ê³„ê°’ë³„ ì •í™•ë„
    thresholds = np.arange(0.3, 1.0, 0.05)
    accuracies = []
    sample_counts = []
    
    for thresh in thresholds:
        high_conf_mask = max_probs >= thresh
        if np.sum(high_conf_mask) > 0:
            high_conf_correct = correct_mask[high_conf_mask]
            accuracy = np.sum(high_conf_correct) / len(high_conf_correct)
            accuracies.append(accuracy)
            sample_counts.append(np.sum(high_conf_mask))
        else:
            accuracies.append(0)
            sample_counts.append(0)
    
    ax_twin = axes[1, 1].twinx()
    
    line1 = axes[1, 1].plot(thresholds, accuracies, 'b-o', label='ì •í™•ë„', linewidth=2)
    line2 = ax_twin.plot(thresholds, sample_counts, 'r-s', label='ìƒ˜í”Œ ìˆ˜', linewidth=2)
    
    axes[1, 1].set_title('í™•ì‹ ë„ ì„ê³„ê°’ë³„ ì •í™•ë„ & ìƒ˜í”Œ ìˆ˜', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('í™•ì‹ ë„ ì„ê³„ê°’', fontsize=12)
    axes[1, 1].set_ylabel('ì •í™•ë„', fontsize=12, color='blue')
    ax_twin.set_ylabel('ìƒ˜í”Œ ìˆ˜', fontsize=12, color='red')
    axes[1, 1].grid(True, alpha=0.3)
    
    # ë²”ë¡€ í†µí•©
    lines1, labels1 = axes[1, 1].get_legend_handles_labels()
    lines2, labels2 = ax_twin.get_legend_handles_labels()
    axes[1, 1].legend(lines1 + lines2, labels1 + labels2, loc='center right')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… í™•ì‹ ë„ ë¶„í¬ ì°¨íŠ¸ê°€ {save_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.show()
    
    # í™•ì‹ ë„ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š í™•ì‹ ë„ ë¶„ì„ ê²°ê³¼:")
    print(f"   â€¢ ì „ì²´ í‰ê·  í™•ì‹ ë„: {np.mean(max_probs):.4f}")
    print(f"   â€¢ ì •ë¶„ë¥˜ í‰ê·  í™•ì‹ ë„: {np.mean(correct_probs):.4f}")
    print(f"   â€¢ ì˜¤ë¶„ë¥˜ í‰ê·  í™•ì‹ ë„: {np.mean(incorrect_probs):.4f}")
    print(f"   â€¢ í™•ì‹ ë„ ì°¨ì´: {np.mean(correct_probs) - np.mean(incorrect_probs):+.4f}")
    
    # ë†’ì€ í™•ì‹ ë„ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤ ë¶„ì„
    high_conf_wrong = (max_probs > 0.8) & (~correct_mask)
    if np.sum(high_conf_wrong) > 0:
        print(f"   â€¢ ë†’ì€ í™•ì‹ ë„(>0.8)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {np.sum(high_conf_wrong)}ê°œ")
    
    # ë‚®ì€ í™•ì‹ ë„ë¡œ ë§ì¶˜ ì¼€ì´ìŠ¤ ë¶„ì„
    low_conf_correct = (max_probs < 0.5) & correct_mask
    if np.sum(low_conf_correct) > 0:
        print(f"   â€¢ ë‚®ì€ í™•ì‹ ë„(<0.5)ë¡œ ë§ì¶˜ ì¼€ì´ìŠ¤: {np.sum(low_conf_correct)}ê°œ")

def analyze_misclassifications(results, top_k=10, save_misclassified=True):
    """ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„ ë° format ì €ì¥"""
    print(f"\nğŸ” ì˜¤ë¶„ë¥˜ ë¶„ì„ (ìƒìœ„ {top_k}ê°œ íŒ¨í„´)")
    print("-" * 60)
    
    # ì˜¤ë¶„ë¥˜ íŒ¨í„´ ìˆ˜ì§‘
    misclass_patterns = defaultdict(int)
    misclassified_data = defaultdict(list)  # ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ì •ë³´
    
    for idx, (true_label, pred_label, format_name) in enumerate(zip(results['labels'], results['predictions'], results['formats'])):
        if true_label != pred_label:
            pattern = f"{LABEL_NAMES[true_label]} â†’ {LABEL_NAMES[pred_label]}"
            misclass_patterns[pattern] += 1
            
            # ì˜¤ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤ ìƒì„¸ ì •ë³´ ì €ì¥ (softmax ê°’ í¬í•¨)
            softmax_probs = results['logits'][idx]  # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ softmax í™•ë¥ 
            misclassified_data[pattern].append({
                'format': format_name,
                'true_label': true_label,
                'pred_label': pred_label,
                'true_label_name': LABEL_NAMES[true_label],
                'pred_label_name': LABEL_NAMES[pred_label],
                'softmax_probs': softmax_probs,
                'pred_confidence': softmax_probs[pred_label],  # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ì˜ í™•ë¥ 
                'true_confidence': softmax_probs[true_label],  # ì‹¤ì œ ì •ë‹µ í´ë˜ìŠ¤ì˜ í™•ë¥ 
                'confidence_diff': softmax_probs[pred_label] - softmax_probs[true_label]  # í™•ë¥  ì°¨ì´
            })
    
    # ìƒìœ„ íŒ¨í„´ ì¶œë ¥
    sorted_patterns = sorted(misclass_patterns.items(), key=lambda x: x[1], reverse=True)
    
    for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
        percentage = (count / len(results['predictions'])) * 100
        print(f"{i+1:2d}. {pattern:<50} : {count:4d}íšŒ ({percentage:.2f}%)")
    
    # ğŸ”¬ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„
    print(f"\nğŸ”¬ ìƒìœ„ {top_k}ê°œ íŒ¨í„´ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„")
    print("=" * 100)
    
    softmax_analysis = {}
    
    for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
        print(f"\n[íŒ¨í„´ {i+1}] {pattern} ({count}ê°œ ì¼€ì´ìŠ¤)")
        print("-" * 80)
        
        cases = misclassified_data[pattern]
        
        # í™•ë¥  í†µê³„ ê³„ì‚°
        pred_confidences = [case['pred_confidence'] for case in cases]
        true_confidences = [case['true_confidence'] for case in cases]
        confidence_diffs = [case['confidence_diff'] for case in cases]
        
        # í†µê³„ ì •ë³´
        stats = {
            'count': count,
            'pred_conf_mean': np.mean(pred_confidences),
            'pred_conf_std': np.std(pred_confidences),
            'pred_conf_min': np.min(pred_confidences),
            'pred_conf_max': np.max(pred_confidences),
            'true_conf_mean': np.mean(true_confidences),
            'true_conf_std': np.std(true_confidences),
            'true_conf_min': np.min(true_confidences),
            'true_conf_max': np.max(true_confidences),
            'diff_mean': np.mean(confidence_diffs),
            'diff_std': np.std(confidence_diffs),
            'high_confidence_wrong': sum(1 for conf in pred_confidences if conf > 0.7),  # ë†’ì€ í™•ì‹ ë„ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤
            'low_confidence_wrong': sum(1 for conf in pred_confidences if conf < 0.4),   # ë‚®ì€ í™•ì‹ ë„ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤
        }
        
        softmax_analysis[pattern] = stats
        
        print(f"ğŸ“Š ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:")
        print(f"   í‰ê· : {stats['pred_conf_mean']:.4f} Â± {stats['pred_conf_std']:.4f}")
        print(f"   ë²”ìœ„: {stats['pred_conf_min']:.4f} ~ {stats['pred_conf_max']:.4f}")
        
        print(f"ğŸ“Š ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:")
        print(f"   í‰ê· : {stats['true_conf_mean']:.4f} Â± {stats['true_conf_std']:.4f}")
        print(f"   ë²”ìœ„: {stats['true_conf_min']:.4f} ~ {stats['true_conf_max']:.4f}")
        
        print(f"ğŸ“Š í™•ë¥  ì°¨ì´ (ì˜ˆì¸¡ - ì‹¤ì œ):")
        print(f"   í‰ê· : {stats['diff_mean']:+.4f} Â± {stats['diff_std']:.4f}")
        
        print(f"ğŸ¯ í™•ì‹ ë„ ë¶„ì„:")
        print(f"   ë†’ì€ í™•ì‹ ë„(>0.7)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['high_confidence_wrong']}/{count} ({stats['high_confidence_wrong']/count*100:.1f}%)")
        print(f"   ë‚®ì€ í™•ì‹ ë„(<0.4)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['low_confidence_wrong']}/{count} ({stats['low_confidence_wrong']/count*100:.1f}%)")
        
        # í™•ë¥  ë¶„í¬ êµ¬ê°„ë³„ ë¶„ì„
        prob_bins = [(0.0, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 1.0)]
        print(f"ğŸ“ˆ ì˜ˆì¸¡ í™•ë¥  êµ¬ê°„ë³„ ë¶„í¬:")
        for low, high in prob_bins:
            count_in_bin = sum(1 for conf in pred_confidences if low <= conf < high)
            percentage_in_bin = count_in_bin / count * 100
            print(f"   {low:.1f}~{high:.1f}: {count_in_bin:3d}ê°œ ({percentage_in_bin:5.1f}%)")
    
    # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    if save_misclassified:
        print(f"\nğŸ’¾ ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
        
        # Softmax ë¶„ì„ ê²°ê³¼ ì €ì¥
        save_file_path = os.path.join(EXPERIMENTS_DIR, 'misclassified_softmax_analysis.txt')
        with open(save_file_path, 'w', encoding='utf-8') as f:
            f.write("ì˜¤ë¶„ë¥˜ ì¼€ì´ìŠ¤ Softmax í™•ë¥  ë¶„í¬ ë¶„ì„\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ì´ ì˜ˆì¸¡ ìˆ˜: {len(results['predictions'])}\n")
            f.write(f"ì´ ì˜¤ë¶„ë¥˜ ìˆ˜: {np.sum(results['labels'] != results['predictions'])}\n")
            f.write(f"ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f}\n\n")
            
            for i, (pattern, count) in enumerate(sorted_patterns[:top_k]):
                stats = softmax_analysis[pattern]
                f.write(f"\n{'='*60}\n")
                f.write(f"[íŒ¨í„´ {i+1}] {pattern} ({count}ê°œ ì¼€ì´ìŠ¤)\n")
                f.write(f"{'='*60}\n")
                
                f.write(f"\nğŸ“Š ì˜ˆì¸¡ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:\n")
                f.write(f"   í‰ê· : {stats['pred_conf_mean']:.4f} Â± {stats['pred_conf_std']:.4f}\n")
                f.write(f"   ë²”ìœ„: {stats['pred_conf_min']:.4f} ~ {stats['pred_conf_max']:.4f}\n")
                
                f.write(f"\nğŸ“Š ì‹¤ì œ í´ë˜ìŠ¤ í™•ë¥  í†µê³„:\n")
                f.write(f"   í‰ê· : {stats['true_conf_mean']:.4f} Â± {stats['true_conf_std']:.4f}\n")
                f.write(f"   ë²”ìœ„: {stats['true_conf_min']:.4f} ~ {stats['true_conf_max']:.4f}\n")
                
                f.write(f"\nğŸ“Š í™•ë¥  ì°¨ì´ (ì˜ˆì¸¡ - ì‹¤ì œ):\n")
                f.write(f"   í‰ê· : {stats['diff_mean']:+.4f} Â± {stats['diff_std']:.4f}\n")
                
                f.write(f"\nğŸ¯ í™•ì‹ ë„ ë¶„ì„:\n")
                f.write(f"   ë†’ì€ í™•ì‹ ë„(>0.7)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['high_confidence_wrong']}/{count} ({stats['high_confidence_wrong']/count*100:.1f}%)\n")
                f.write(f"   ë‚®ì€ í™•ì‹ ë„(<0.4)ë¡œ í‹€ë¦° ì¼€ì´ìŠ¤: {stats['low_confidence_wrong']}/{count} ({stats['low_confidence_wrong']/count*100:.1f}%)\n")
                
                # ìƒì„¸ ì¼€ì´ìŠ¤ë³„ ì •ë³´
                f.write(f"\nğŸ“‹ ìƒì„¸ ì¼€ì´ìŠ¤ë³„ í™•ë¥  ì •ë³´:\n")
                f.write(f"{'No.':<4} {'Format':<30} {'ì˜ˆì¸¡í™•ë¥ ':<8} {'ì‹¤ì œí™•ë¥ ':<8} {'ì°¨ì´':<8}\n")
                f.write("-" * 60 + "\n")
                
                cases = misclassified_data[pattern]
                # ì˜ˆì¸¡ í™•ë¥ ì´ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_cases = sorted(cases, key=lambda x: x['pred_confidence'], reverse=True)
                
                for j, case in enumerate(sorted_cases[:20]):  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                    f.write(f"{j+1:<4} {case['format']:<30} {case['pred_confidence']:<8.4f} {case['true_confidence']:<8.4f} {case['confidence_diff']:+8.4f}\n")
                
                if len(sorted_cases) > 20:
                    f.write(f"... (ì´ {len(sorted_cases)}ê°œ ì¤‘ ìƒìœ„ 20ê°œë§Œ í‘œì‹œ)\n")
        
        print(f"   âœ… {save_file_path}: Softmax í™•ë¥  ë¶„í¬ ìƒì„¸ ë¶„ì„")
   

# ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
def run_comprehensive_evaluation(model, dataloader, device, save_plots=True):
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€ ì‹¤í–‰"""
    print("ğŸš€ ì¢…í•© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    results = evaluate_model_performance(model, dataloader, device)
    
    # ë³´ê³ ì„œ ì¶œë ¥
    print_performance_report(results)
    
    # ì˜¤ë¶„ë¥˜ ë¶„ì„
    analyze_misclassifications(results)
    
    if save_plots:
        # ê¸°ì¡´ ì‹œê°í™”
        plot_confusion_matrix(results['confusion_matrix'])
        plot_class_performance(results)
        
        # ìƒˆë¡œìš´ ì‹œê°í™” ì¶”ê°€
        plot_tsne_visualization(results, input_type='features', max_samples=5000)  # Feature vectors ì‚¬ìš© (ê¶Œì¥)
        plot_tsne_visualization(results, input_type='raw_logits', max_samples=5000)  # Raw logits ì‚¬ìš©
        plot_confidence_distribution(results)
        
        # ë¹„êµ ë¶„ì„ (ì„ íƒì‚¬í•­)
        compare_tsne_inputs(results)
    
    # ìš”ì•½ í†µê³„
    print(f"\nğŸ“Œ ìš”ì•½ í†µê³„:")
    print(f"   â€¢ ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.4f}")
    print(f"   â€¢ í‰ê·  F1-ì ìˆ˜: {np.mean(results['f1_score']):.4f}")
    print(f"   â€¢ ê°€ì¤‘ F1-ì ìˆ˜: {np.average(results['f1_score'], weights=results['support']):.4f}")
    print(f"   â€¢ ì´ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(results['predictions'])}")
    print(f"   â€¢ ì´ ì˜¤ë¶„ë¥˜: {np.sum(results['labels'] != results['predictions'])}")
    
    return results

# ê¸°ì¡´ ì‹¤í—˜ ì½”ë“œë¥¼ í™•ì¥
def run_basic_test(model, dataloader, device):
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì½”ë“œ)"""
    print("ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
    
    for batch in dataloader:
        landmarks = batch['landmarks'].to(device)
        label = batch['label'].long().to(device)
        logits = model(landmarks)
        _, predicted = torch.max(logits.data, 1)
        print("ì˜ˆì¸¡ê°’:", predicted)
        print("ì‹¤ì œê°’:", label)
        break

def test_experiment():
    model = PointNetClassifier(num_classes=5)
    
    # ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(torch.load(MODEL_PATH, map_location='cpu'))
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Validation ë°ì´í„°ì…‹ ë¡œë“œ (ì´ë¯¸ ê²€ì¦ìš© ë°ì´í„°)
    test_dataset = FocusDataset_multi(base_path)
    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    print(f"Validation ë°ì´í„°ì…‹ í¬ê¸°: {len(test_dataset)}")
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ ì½”ë“œ)
    print("\n" + "="*50)
    print("1. ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
    print("="*50)
    run_basic_test(model, test_dataloader, device)
    
    # ì¢…í•©ì ì¸ ì„±ëŠ¥ í‰ê°€
    print("\n" + "="*50)  
    print("2. ì¢…í•© ì„±ëŠ¥ í‰ê°€")
    print("="*50)
    results = run_comprehensive_evaluation(model, test_dataloader, device, save_plots=True)
    
    # ì¶”ê°€ ë¶„ì„ (ì„ íƒì‚¬í•­)
    print("\n" + "="*50)
    print("3. ì¶”ê°€ ë¶„ì„")
    print("="*50)
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„
    class_distribution = results['class_total']
    total_samples = sum(class_distribution.values())
    
    print("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„:")
    for i in range(5):
        count = class_distribution.get(i, 0)
        percentage = (count / total_samples) * 100
        print(f"   {LABEL_NAMES[i]}: {count}ê°œ ({percentage:.1f}%)")
    
    # ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€/ë‚˜ìœ í´ë˜ìŠ¤
    f1_scores = results['f1_score']
    best_class = np.argmax(f1_scores)
    worst_class = np.argmin(f1_scores)
    
    print(f"\nğŸ† ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ í´ë˜ìŠ¤: {LABEL_NAMES[best_class]} (F1: {f1_scores[best_class]:.4f})")
    print(f"ğŸš¨ ì„±ëŠ¥ì´ ê°€ì¥ ë‚˜ìœ í´ë˜ìŠ¤: {LABEL_NAMES[worst_class]} (F1: {f1_scores[worst_class]:.4f})")
    
    return results

if __name__ == "__main__":
    print("ğŸš€ FocusSu AI ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # matplotlib ë°±ì—”ë“œ ì„¤ì •
    import matplotlib

    
    # ì‹¤í—˜ ì‹¤í–‰
    print("\n" + "="*60)
    results = test_experiment()
    
    print("\n" + "="*80)
    print("ğŸ‰ ì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ìƒì„±ëœ íŒŒì¼:")
    print(f"  ğŸ“Š ì‹œê°í™” (ì €ì¥ ìœ„ì¹˜: {EXPERIMENTS_DIR}/)")
    print("    - confusion_matrix.png: í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ")
    print("    - class_performance.png: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ì°¨íŠ¸")
    print("    - tsne_raw_logits_visualization.png: Raw logits ì‚¬ìš©í•œ t-SNE ì‹œê°í™”")
    print("    - tsne_softmax_visualization.png: Softmax í™•ë¥  ì‚¬ìš©í•œ t-SNE ì‹œê°í™”")
    print("    - confidence_distribution.png: ì˜ˆì¸¡ í™•ì‹ ë„ ë¶„í¬ ë¶„ì„")
    print("  ğŸ“„ ì˜¤ë¶„ë¥˜ ë¶„ì„:")
    print("    - misclassified_softmax_analysis.txt: Softmax í™•ë¥  ë¶„í¬ ìƒì„¸ ë¶„ì„")
    print("="*80)