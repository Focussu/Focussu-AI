{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 01:12:15.492973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747185135.519414    3591 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747185135.525573    3591 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747185135.550781    3591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747185135.550807    3591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747185135.550809    3591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747185135.550811    3591 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 01:12:15.560676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ëˆˆ ê°ì•˜ëŠ”ì§€ ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_EAR(landmarks, eye_indices, image_w, image_h):\n",
    "    points = [(int(landmarks[i].x * image_w), int(landmarks[i].y * image_h)) for i in eye_indices]\n",
    "    A = np.linalg.norm(np.array(points[1]) - np.array(points[5]))\n",
    "    B = np.linalg.norm(np.array(points[2]) - np.array(points[4]))\n",
    "    C = np.linalg.norm(np.array(points[0]) - np.array(points[3]))\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì™¼ìª½, ì˜¤ë¥¸ìª½ ëˆˆ ì¸ë±ìŠ¤\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAR(landmarks, image_w, image_h):\n",
    "    top = np.array([landmarks[13].x * image_w, landmarks[13].y * image_h])\n",
    "    bottom = np.array([landmarks[14].x * image_w, landmarks[14].y * image_h])\n",
    "    left = np.array([landmarks[78].x * image_w, landmarks[78].y * image_h])\n",
    "    right = np.array([landmarks[308].x * image_w, landmarks[308].y * image_h])\n",
    "    \n",
    "    vertical = np.linalg.norm(top - bottom)\n",
    "    horizontal = np.linalg.norm(left - right)\n",
    "    \n",
    "    return vertical / horizontal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‘œì • íŒë‹¨\n",
    "def classify_expression(ear, mar):\n",
    "    if ear < 0.20 and mar < 0.3:\n",
    "        return \"ì°¨ë¶„í•¨\"\n",
    "    elif ear > 0.23 or mar > 0.35:\n",
    "        return \"í¥ë¯¸ë¡œì›€\"\n",
    "    else:\n",
    "        return \"ì°¨ë¶„í•¨\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "path = \"/workspaces/Focussu-AI/data/WIN_20250512_22_01_55_Pro.jpg\"\n",
    "image = cv2.imread(path)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê°ì • ì¸ì‹ ê²°ê³¼: ì°¨ë¶„í•¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747186123.491426    5943 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747186123.519627    5942 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "with mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True) as face_mesh:\n",
    "    results = face_mesh.process(rgb)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "\n",
    "            ear_left = calculate_EAR(landmarks, LEFT_EYE, w, h)\n",
    "            ear_right = calculate_EAR(landmarks, RIGHT_EYE, w, h)\n",
    "            ear_avg = (ear_left + ear_right) / 2.0\n",
    "\n",
    "            mar = calculate_MAR(landmarks, w, h)\n",
    "\n",
    "            expression = classify_expression(ear_avg, mar)\n",
    "            print(\"ê°ì • ì¸ì‹ ê²°ê³¼:\", expression)\n",
    "    else:\n",
    "        print(\"ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAR: 0.332\n",
      "ğŸ‘ï¸ ëˆˆ ë–  ìˆìŒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747185161.539927    3738 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747185161.558936    3738 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ëˆˆ ë–´ëŠ”ì§€ íŒë‹¨ë‹¨\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True, refine_landmarks=True) as face_mesh:\n",
    "    results = face_mesh.process(rgb)\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = face_landmarks.landmark\n",
    "            \n",
    "            left_EAR = calculate_EAR(landmarks, LEFT_EYE, w, h)\n",
    "            right_EAR = calculate_EAR(landmarks, RIGHT_EYE, w, h)\n",
    "\n",
    "            avg_EAR = (left_EAR + right_EAR) / 2.0\n",
    "            print(f\"EAR: {avg_EAR:.3f}\")\n",
    "\n",
    "            if avg_EAR < 0.20:\n",
    "                print(\"ğŸ”’ ëˆˆ ê°ì•˜ìŒ!\")\n",
    "            else:\n",
    "                print(\"ğŸ‘ï¸ ëˆˆ ë–  ìˆìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747184804.745790    2779 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747184804.791982    2778 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ \n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,       # ëˆˆ, ì…ìˆ  ë””í…Œì¼ í–¥ìƒ\n",
    "    min_detection_confidence=0.5\n",
    ") as face_mesh:\n",
    "    results = face_mesh.process(rgb)\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, landmark in enumerate(face_landmarks.landmark):\n",
    "                h, w, _ = image.shape\n",
    "                x = int(landmark.x * w)\n",
    "                y = int(landmark.y * h)\n",
    "                cv2.circle(image, (x, y), 1, (0, 255, 0), -1)\n",
    "    else:\n",
    "        print(\"ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Face Landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1747184831.458745    2868 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747184831.496243    2867 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "left_iris = [468, 469, 470, 471, 472]\n",
    "right_iris = [473, 474, 475, 476, 477]\n",
    "\n",
    "# ëˆˆ, ë™ê³µë§Œ ëœë“œë§ˆí¬ ì¶”ì¶œ \n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,       # ëˆˆ, ì…ìˆ  ë””í…Œì¼ í–¥ìƒ\n",
    "    min_detection_confidence=0.5\n",
    ") as face_mesh:\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = image.shape\n",
    "            for idx in left_iris + right_iris:\n",
    "                landmark = face_landmarks.landmark[idx]\n",
    "                x = int(landmark.x * w)\n",
    "                y = int(landmark.y * h)\n",
    "                cv2.circle(image, (x, y), 2, (0, 0, 255), -1)  # ë¹¨ê°„ ì \n",
    "    else:\n",
    "        print(\"ì–¼êµ´ì„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Face Landmarks\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
