from fastapi import FastAPI, HTTPException, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from tqdm import tqdm

import os
from pathlib import Path
import torch
import numpy as np
from PIL import Image
from dotenv import load_dotenv
import requests
import io
from inference.pointnet import load_pointnet, predict
from inference.random_forest import load_random_forest, predict_rf
from datetime import datetime

# LLaVA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from LLaVA.llava.model.builder import load_pretrained_model
from LLaVA.llava.constants import IMAGE_TOKEN_INDEX
from LLaVA.llava.conversation import conv_templates
from LLaVA.llava.mm_utils import process_images, tokenizer_image_token
from sentence_transformers import SentenceTransformer, util
import openai
import shutil

from collections import defaultdict


load_dotenv()

BATCH_SIZE = 4  # ë°°ì¹˜ í¬ê¸° ì„¤ì •

API_SERVER_URL = os.getenv('API_SERVER_URL')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

class UserScore():
    def __init__(self):
        self.userScore = defaultdict(list)

    def addScore(self, ticketNumber, score):
        scores = self.userScore[ticketNumber]
        
        # ìµœê·¼ 5ê°œë§Œ ìœ ì§€
        if len(scores) >= 5:
            scores.pop(0)
        scores.append(score)

        # ê¸‰ê²©í•œ ë³€í™” ê°ì§€ (ìƒìŠ¹/í•˜ë½ í†µí•©)
        if len(scores) >= 3:
            delta = abs(scores[-1] - scores[-2])
            if delta >= 0.25:
                print("ğŸ“ˆ ê¸‰ê²©í•œ ë³€í™” ê°ì§€")
                return True
            delta2 = abs(scores[-1]-scores[0])
            if delta2 >= 0.25:
                print("ğŸ“ˆ ê¸‰ê²©í•œ ë³€í™” ê°ì§€")
                return True
        else:
            return False



        # ì €ì  (ì§‘ì¤‘ ë§¤ìš° ë‚®ìŒ)
        if scores[-1] < 0.3 and scores[-2] < 0.3:
            print("ğŸŸ  ì§‘ì¤‘ë„ ì €ì ")
            return True
        if all(score > 0.8 for score in scores):
            print("ğŸŸ¢ ì§‘ì¤‘ë„ ìµœê³ ì ")
            return True
        return False


userScore = UserScore()

# Pydantic ëª¨ë¸ ì •ì˜
class ImageUploadResponse(BaseModel):
    filename: str
    content_type: str
    size: int
    message: str
    saved_path: str

class LandmarkData(BaseModel):
    x: float
    y: float
    z: float

class BlendshapeData(BaseModel):
    categoryName: str
    score: float

class ScorePredictionRequest(BaseModel):
    ticketNumber: int
    landmarks: Any  # MediaPipe êµ¬ì¡°ì— ë§ê²Œ Anyë¡œ ë³€ê²½
    blendshapes: Optional[Any] = None  # MediaPipe êµ¬ì¡°ì— ë§ê²Œ Anyë¡œ ë³€ê²½
    timestamp: int

class ScoreResponse(BaseModel):
    landmark_score: float
    blendshape_score: float
    confidence: float
    processing_time: float
    flag: bool

class ErrorResponse(BaseModel):
    detail: str

# FastAPI ì•± ìƒì„± (ë©”íƒ€ë°ì´í„° ì¶”ê°€)
app = FastAPI(
    title="FocusSu AI API",
    description="ì–¼êµ´ ì´ë¯¸ì§€ ë¶„ì„ ë° ì˜ˆì¸¡ì„ ìœ„í•œ AI ë°±ì—”ë“œ API",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI ê²½ë¡œ
    redoc_url="/redoc",  # ReDoc ê²½ë¡œ
    openapi_url="/openapi.json"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],  # ëª¨ë“  HTTP ë©”ì„œë“œ í—ˆìš©
    allow_headers=["*"],  # ëª¨ë“  í—¤ë” í—ˆìš©
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì • (src ë””ë ‰í† ë¦¬ì˜ ìƒìœ„ ë””ë ‰í† ë¦¬)
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
MODEL_DIR = PROJECT_ROOT / "model"

# ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
DATA_DIR.mkdir(exist_ok=True)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
POINTNET_MODEL_PATH = MODEL_DIR / "best_multi_model.pth"
RF_MODEL_PATH = MODEL_DIR / "random_forest_blendshape.pkl"

# ëª¨ë¸ ë¡œë“œ
try:
    POINTNET_MODEL = load_pointnet(POINTNET_MODEL_PATH, DEVICE)
    RF_MODEL = load_random_forest(RF_MODEL_PATH, DEVICE)
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    raise

# API ë¼ìš°íŒ…
@app.get("/", 
         summary="ì„œë²„ ìƒíƒœ í™•ì¸",
         description="API ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
         response_model=Dict[str, str])
def read_root():
    return {"message": "AI backend ì„œë²„ì…ë‹ˆë‹¤."}

@app.post("/image",
          summary="ì´ë¯¸ì§€ ì—…ë¡œë“œ",
          description="ì´ë¯¸ì§€ íŒŒì¼ì„ ì„œë²„ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ì§€ì› í˜•ì‹: JPG, PNG, GIF ë“±",
          response_model=ImageUploadResponse,
          responses={
              400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ íŒŒì¼ í˜•ì‹"},
              500: {"model": ErrorResponse, "description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜"}
          })
async def image_upload(file: UploadFile = File(..., description="ì—…ë¡œë“œí•  ì´ë¯¸ì§€ íŒŒì¼")):
    # íŒŒì¼ì´ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸
    if not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    try:
        # ì´ë¯¸ì§€ ë°ì´í„° ì½ê¸°
        contents = await file.read()
        
        # íŒŒì¼ëª…ì—ì„œ ticketNumberì™€ timestamp ì¶”ì¶œ ì‹œë„
        try:
            file_name = file.filename.split(".")[0]
            ticketNumber, timestamp = file_name.split("_")
            ticketNumber = int(ticketNumber)
        except (ValueError, AttributeError, IndexError):
            # íŒŒì¼ëª…ì´ ì˜ˆìƒ í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            ticketNumber = 0
            timestamp = "unknown"
            print(f"íŒŒì¼ëª… '{file.filename}'ì´ ì˜ˆìƒ í˜•ì‹(ticketNumber_timestamp.í™•ì¥ì)ì´ ì•„ë‹™ë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # íŒŒì¼ ì €ì¥
        # ticketNumberë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        ticket_dir = DATA_DIR / str(ticketNumber)
        ticket_dir.mkdir(exist_ok=True)  # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        
        file_path = ticket_dir / file.filename
        with open(file_path, "wb") as f:
            f.write(contents)
        
        return ImageUploadResponse(
            filename=file.filename,
            content_type=file.content_type,
            size=len(contents),
            message="ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„±ê³µ",
            saved_path=str(file_path)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
openai.api_key = OPENAI_API_KEY

# ====== ëª¨ë¸ ë¡œë“œ ======
print("Loading LLaVA model...")
tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path="/home/focussu/minji/Focussu-AI/src/LLaVA/checkpoints/llava-v1.5-7b",
    model_base=None,
    model_name="llava-v1.5-7b",
    device_map="cuda",
    load_8bit=True,
    load_4bit=False
)
device = next(model.parameters()).device
dtype = next(model.parameters()).dtype
print(f"Model loaded on device: {device}, dtype: {dtype}")

#app = FastAPI()

# ====== ì „ì—­ í…œí”Œë¦¿ ë° ìƒìˆ˜ ======
conv_template = conv_templates["llava_v1"].copy()
IMAGE_TOKEN_INDEX = tokenizer.convert_tokens_to_ids(["<image>"])[0]  # í•„ìš”í•œ ê²½ìš° ì§ì ‘ ì„¤ì •
base_prompt = (
    "This is a frame taken from a video recorded every 10 seconds. "
    "Please analyze the person's posture, gaze direction, and activity. "
    "Evaluate their level of concentration and explain why you think so. "
    "If the person seems distracted, suggest possible causes."
)

# ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
def load_images_from_folder(folder_path: str) -> List[Image.Image]:
    image_list = []
    for filename in sorted(os.listdir(folder_path)):
        if filename.lower().endswith((".png", ".jpg", ".jpeg")):
            image_path = os.path.join(folder_path, filename)
            img = Image.open(image_path).convert("RGB")
            image_list.append(img)
    return image_list

# í”„ë¡¬í”„íŠ¸ ìƒì„±
def make_prompts(start_idx: int, end_idx: int) -> List[str]:
    return [f"{base_prompt}\n\nThis is frame {i+1} in the sequence." for i in range(start_idx, end_idx)]


@app.post("/analyze")
async def analyze_concentration(ticketNumber: int, userID: int):
# async def analyze_concentration(files: List[UploadFile] = File(...)):
    print(f"ì§‘ì¤‘ë„ ë¶„ì„ ì‹œì‘ - ticketNumber: {ticketNumber}, userID: {userID}")
    try:
        folder_path = f"/home/focussu/minji/Focussu-AI/data/{ticketNumber}"
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {folder_path}")

        # 1. ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
        image_list = load_images_from_folder(folder_path)
        if len(image_list) == 0:
            raise ValueError("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        image_tensor = process_pil_images(image_list)
        
        # 2. ë°°ì¹˜ë¡œ ë‚˜ëˆ  ì²˜ë¦¬
        all_responses = []
        num_images = image_tensor.shape[0]
        for i in tqdm(range(0, num_images, BATCH_SIZE)):
            image_batch = image_tensor[i:i+BATCH_SIZE]
            prompts = make_prompts(i, min(i + BATCH_SIZE, num_images))
            batch_responses = generate_responses(image_batch, prompts)
            all_responses.extend(batch_responses)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_responses = deduplicate_responses(all_responses)
        translated = translate_with_gpt(unique_responses)


        # Post (ticketNumber, content)
        
        #    API ì„œë²„ë¡œ ë¶„ì„ ê²°ê³¼ ì „ì†¡ (í—¤ë” ì¶”ê°€)
        headers = {
            'Content-Type': 'application/json',
            #'Authorization': f'Bearer {API_TOKEN}'  # Bearer í† í° ì¶”ê°€
        }
        
        payload = {
            "ticketNumber": ticketNumber,
            # "userID": userID,
            "content": translated,
        }
        
        response = requests.post(
            f"{API_SERVER_URL}/analysis-document", 
            json=payload,  # json íŒŒë¼ë¯¸í„° ì‚¬ìš©
            headers=headers
        )
        print(response)
        
        return {
            "status": "success",
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ====== ìœ í‹¸ í•¨ìˆ˜ë“¤ ======
def process_uploaded_images(files: List[UploadFile]) -> torch.Tensor:
    images = [Image.open(io.BytesIO(file.file.read())).convert("RGB") for file in files]
    image_tensor = process_images(images, image_processor, model.config)
    return image_tensor.to(device=device, dtype=dtype)

def process_pil_images(images: List[Image.Image]) -> torch.Tensor:
    """PIL Image ë¦¬ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ í…ì„œë¡œ ë³€í™˜"""
    image_tensor = process_images(images, image_processor, model.config)
    return image_tensor.to(device=device, dtype=dtype)

def generate_responses(image_tensor_batch: torch.Tensor, prompts: List[str]) -> List[str]:
    conv_batch = [conv_template.copy() for _ in prompts]
    for conv, prompt in zip(conv_batch, prompts):
        conv.messages = []
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], None)

    input_ids_batch = [
        tokenizer_image_token(c.get_prompt(), tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
        for c in conv_batch
    ]
    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_batch, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)

    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            images=image_tensor_batch,
            do_sample=False,
            temperature=0.2,
            max_new_tokens=256,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # ì‘ë‹µ ë””ì½”ë”©
    return [
        tokenizer.decode(output_ids[i][input_ids.shape[1]:], skip_special_tokens=True).strip()
        for i in range(output_ids.shape[0])
    ]


def deduplicate_responses(paragraphs: List[str], threshold: float = 0.85) -> List[str]:
    model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
    embeddings = model.encode(paragraphs, convert_to_tensor=True)
    unique_paragraphs = []

    for i, emb in enumerate(embeddings):
        if all(util.cos_sim(emb, model.encode([p], convert_to_tensor=True)).item() < threshold for p in unique_paragraphs):
            unique_paragraphs.append(paragraphs[i])
    return unique_paragraphs


def translate_with_gpt(sentences: List[str]) -> str:
    prompt = (
        "ë‹¤ìŒ ë¬¸ì¥ë“¤ì€ ì˜ìƒ ì† ì‚¬ëŒ í•œ ëª…ì˜ ì‹œê°„ì— ë”°ë¥¸ ì§‘ì¤‘ë„, ìì„¸, ì‹œì„ ì„ ë¶„ì„í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n"
        "ì´ ë¬¸ì¥ë“¤ì„ ì¤‘ë³µ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³ , ë¬¸ë§¥ì´ ì–´ìƒ‰í•œ ë¶€ë¶„ì´ë‚˜ ë§ì´ ì´ì–´ì§€ì§€ ì•ŠëŠ” ë¶€ë¶„ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ ì£¼ì„¸ìš”. ë¬¸ì¥ ì•ì— ë„˜ë²„ë§ì€ ë§¤ê¸°ì§€ ë§ì•„ì£¼ì„¸ìš”.\n\n"
    )
    joined = "\n".join(f"{i+1}. {s}" for i, s in enumerate(sentences))

    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt + joined}],
        temperature=0.2,
    )
    return response.choices[0].message["content"].strip()

@app.post("/ai/score",
          summary="ì–¼êµ´ ë¶„ì„ ë° ì˜ˆì¸¡",
          description="í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡í•œ ëœë“œë§ˆí¬ì™€ ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ AI ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
          response_model=ScoreResponse,
          responses={
              400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ë°ì´í„° í˜•ì‹"},
              500: {"model": ErrorResponse, "description": "ì˜ˆì¸¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜"}
          })
async def get_score(request: ScorePredictionRequest):
    try:
        import time
        start_time = time.time()
        ticketNumber = request.ticketNumber if request.ticketNumber else 0
        # ëœë“œë§ˆí¬ ë°ì´í„° ì²˜ë¦¬
        if not request.landmarks:
            raise HTTPException(status_code=400, detail="ëœë“œë§ˆí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        print(f"ticketNumber: {ticketNumber}")
        landmarks_tensor = _process_landmarks(request.landmarks)

        
        # ë¸”ë Œë“œì…°ì´í”„ ê¸°ë°˜ ì˜ˆì¸¡ (ì„ íƒì ) - ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš©
        if not request.blendshapes:
            raise HTTPException(status_code=400, detail="ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        blendshapes_tensor = _process_blendshapes(request.blendshapes)
        if blendshapes_tensor is None:
            raise HTTPException(status_code=400, detail="ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨")
              
        # ëœë“œë§ˆí¬ ê¸°ë°˜ ì˜ˆì¸¡ (PointNet) - ë°°ì¹˜ ì²˜ë¦¬
        landmark_result = predict(POINTNET_MODEL, landmarks_tensor, DEVICE)
        
        # ë¸”ë Œë“œì…°ì´í”„ ê¸°ë°˜ ì˜ˆì¸¡ (Random Forest) - ë°°ì¹˜ ì²˜ë¦¬
        blendshape_result = predict_rf(RF_MODEL, blendshapes_tensor)
        
        

        # PyTorch í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ í‰ê·  ê³„ì‚°
        if isinstance(landmark_result, torch.Tensor):
            landmark_result = landmark_result.detach().cpu().numpy()
        
        landmark_result = np.mean(landmark_result, axis=0)
        pos = landmark_result[0] + landmark_result[1]
        neg = landmark_result[2] + landmark_result[3]
        #  0: ì§‘ì¤‘(í¥ë¯¸) 1: ì§‘ì¤‘(ì°¨ë¶„) 2: ë¹„ì§‘ì¤‘(ì°¨ë¶„) 3: ë¹„ì§‘ì¤‘(ì¡¸ìŒ) 4: ì¡¸ìŒ
        if landmark_result[4] > 0.5:
            landmark_score = 0.0
        else:
            landmark_score = pos if pos > neg else (1-neg)
        
        if isinstance(blendshape_result, (list, tuple, np.ndarray)):
            blendshape_score = float(np.mean(blendshape_result))
        else:
            blendshape_score = float(blendshape_result)
                
    
        # ìµœì¢… confidence ê³„ì‚°
        final_confidence = (0.8 * landmark_score + 0.2 * blendshape_score) if blendshape_score > 0 else landmark_score
        processing_time = time.time() - start_time
        end_time = time.time()

        # ì‹œê°„ì„ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        start_time_iso = datetime.fromtimestamp(start_time).isoformat() + 'Z'
        end_time_iso = datetime.fromtimestamp(end_time).isoformat() + 'Z'
        
        # API ì„œë²„ë¡œ ë¶„ì„ ê²°ê³¼ ì „ì†¡ (í—¤ë” ì¶”ê°€)
        headers = {
            'Content-Type': 'application/json',
            #'Authorization': f'Bearer {API_TOKEN}'  # Bearer í† í° ì¶”ê°€
        }
        
        payload = {
            "ticketNumber": ticketNumber,
            "startTime": start_time_iso,
            "endTime": end_time_iso,
            "score": final_confidence
        }
        
        response = requests.post(
            f"{API_SERVER_URL}/ai-analysis", 
            json=payload,  # json íŒŒë¼ë¯¸í„° ì‚¬ìš©
            headers=headers
        )
        print(response)

        # ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ ë° ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€ ì„¤ì •
        flag = userScore.addScore(ticketNumber, final_confidence)

        return ScoreResponse(
            landmark_score=landmark_score,
            blendshape_score=blendshape_score,
            confidence=float(final_confidence),
            processing_time=round(processing_time, 3),
            flag=flag
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"ì˜ˆì¸¡ ì¤‘ ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def _process_landmarks(landmarks_data):
    """ëœë“œë§ˆí¬ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ 3ì°¨ì› í…ì„œë¡œ ë³€í™˜ (B, 478, 3)"""
    
    # landmarks_dataê°€ ë°°ì¹˜ë¡œ ì „ë‹¬ë˜ëŠ” ê²½ìš°ë¥¼ ê°€ì •
    # ì…ë ¥ í˜•íƒœ: [batch_of_faces] ë˜ëŠ” [face1, face2, ...] 
    if isinstance(landmarks_data, list) and len(landmarks_data) > 0:
        batch_landmarks = []
        
        # ê° ë°°ì¹˜ í•­ëª© ì²˜ë¦¬
        for batch_item in landmarks_data:
            if isinstance(batch_item, list) and len(batch_item) > 0:
                # ë‹¨ì¼ ì–¼êµ´ì˜ ëœë“œë§ˆí¬ ë¦¬ìŠ¤íŠ¸
                face_landmarks_array = []
                for lm in batch_item[0]:
                    if isinstance(lm, dict):
                        face_landmarks_array.append([lm.get('x', 0.0), lm.get('y', 0.0), lm.get('z', 0.0)])
                    else:
                        # ê°ì²´ í˜•íƒœë¼ë©´ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
                        face_landmarks_array.append([getattr(lm, 'x', 0.0), getattr(lm, 'y', 0.0), getattr(lm, 'z', 0.0)])
                
                batch_landmarks.append(face_landmarks_array)
            else:
                print("ëœë“œë§ˆí¬ ë°°ì¹˜ í•­ëª©ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°.")
                # ì˜¬ë°”ë¥´ì§€ ì•Šì€ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸°
                continue
        
        if not batch_landmarks:
            raise HTTPException(status_code=400, detail="ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëœë“œë§ˆí¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # (B, 478, 3) í˜•íƒœì˜ í…ì„œ ìƒì„±
        landmarks_tensor = torch.tensor(batch_landmarks, dtype=torch.float32)
        print(f"ë°°ì¹˜ ì²˜ë¦¬ëœ ëœë“œë§ˆí¬ í˜•íƒœ: {landmarks_tensor.shape}")
        return landmarks_tensor
    
    else:
        raise HTTPException(status_code=400, detail="ëœë“œë§ˆí¬ ë°ì´í„°ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¬ë°”ë¥´ì§€ ì•Šì€ í˜•ì‹ì…ë‹ˆë‹¤")

def _process_blendshapes(blendshapes_data):
    """ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë°°ì¹˜ í…ì„œë¡œ ë³€í™˜ (B, 52)"""
    try:
        #type(blendshapes_data): class 'list'
        
        # MediaPipe ë¸”ë Œë“œì…°ì´í”„ êµ¬ì¡°: [{ categories: [{ score: float, categoryName: string }, ...] }]
        batch_blendshapes = []
        
        if isinstance(blendshapes_data, list) and len(blendshapes_data) > 0:
            for batch_item in blendshapes_data:
                if isinstance(batch_item, list) and len(batch_item) > 0:
                    # ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ë¸”ë Œë“œì…°ì´í”„ ì‚¬ìš©
                    face_blendshapes = batch_item[0]

                    # categories í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if isinstance(face_blendshapes, dict) and 'categories' in face_blendshapes:
                        categories = face_blendshapes['categories']
                
                        blendshape_scores = []
           
                        for i, category in enumerate(categories): 
                            if isinstance(category, dict):
                                score = category.get('score', 0.0)
                                category_name = category.get('categoryName', f'unknown_{i}')
                                blendshape_scores.append(score)
                                # if i < 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
                                #     print(f"  {i}: {category_name} = {score:.6f}")
                            else:
                                # ê°ì²´ í˜•íƒœë¼ë©´ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
                                score = getattr(category, 'score', 0.0)
                                blendshape_scores.append(score)
                        
                        batch_blendshapes.append(blendshape_scores)
                    else:
                        print(f"ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ì— 'categories' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë°°ì¹˜ í•­ëª© ê±´ë„ˆë›°ê¸°.")
                        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(face_blendshapes.keys()) if isinstance(face_blendshapes, dict) else 'not dict'}")
                        # ì˜¬ë°”ë¥´ì§€ ì•Šì€ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ë¹ˆ ë°ì´í„° ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
                        continue
                else:
                    print("ë°°ì¹˜ í•­ëª©ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆë›°ê¸°.")
                    # ì˜¬ë°”ë¥´ì§€ ì•Šì€ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸° (ë¹ˆ ë°ì´í„° ì¶”ê°€í•˜ì§€ ì•ŠìŒ)
                    continue
        
        if not batch_blendshapes:
            print("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¸”ë Œë“œì…°ì´í”„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        # (B, 52) í˜•íƒœì˜ í…ì„œ ìƒì„±
        blendshapes_tensor = torch.tensor(batch_blendshapes, dtype=torch.float32)
        
        return blendshapes_tensor
        
    except Exception as e:
        print(f"ë¸”ë Œë“œì…°ì´í”„ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        import traceback
        print(traceback.format_exc())
        return None


@app.get("/health",
         summary="í—¬ìŠ¤ ì²´í¬",
         description="ì„œë²„ì™€ ëª¨ë¸ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
         response_model=Dict[str, Any])
def health_check():
    return {
        "status": "healthy",
        "device": str(DEVICE),
        "models_loaded": {
            "pointnet": POINTNET_MODEL is not None,
            "random_forest": RF_MODEL is not None
        }
    }